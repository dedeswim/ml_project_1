{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from src.helpers import load_csv_data, standardize, flatten_jet_features, predict_labels, create_csv_submission\n",
    "from src.logistic.implementations import logistic_regression\n",
    "from src.logistic.loss import compute_loss\n",
    "from src.logistic.not_req_impl import reg_logistic_regression, gradient_descent_step\n",
    "from src.logistic.gradient import compute_gradient\n",
    "from src.split import split_data\n",
    "from src.polynomials import build_poly_matrix_vandermonde\n",
    "\n",
    "from src.logistic.sigmoid import sigmoid\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = flatten_jet_features(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize x and add the 1s column\n",
    "x, mean_x, std_x = standardize(x)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "tx_poly = build_poly_matrix_vandermonde(tx, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "tx_train, y_train, tx_test, y_test = split_data(tx_poly, y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[271210.8712248]\n",
      "||d|| = 28.187006321784594\n",
      "Current iteration=1000, loss=[473276.92933884]\n",
      "||d|| = 0.0018141404419597484\n",
      "Current iteration=2000, loss=[471457.80712879]\n",
      "||d|| = 72.80105748739423\n",
      "Current iteration=3000, loss=[413814.03478361]\n",
      "||d|| = 2.20590705580624\n",
      "Current iteration=4000, loss=[382755.89768559]\n",
      "||d|| = 0.0011342766214547798\n",
      "Current iteration=5000, loss=[362612.0083851]\n",
      "||d|| = 6.656985821296442\n",
      "Current iteration=6000, loss=[347606.82277806]\n",
      "||d|| = 3.1595568788074604\n",
      "Current iteration=7000, loss=[325844.92175665]\n",
      "||d|| = 17.07173239625616\n",
      "Current iteration=8000, loss=[313553.25590884]\n",
      "||d|| = 3.19607849012299\n",
      "Current iteration=9000, loss=[295879.7846367]\n",
      "||d|| = 2.085721188752055\n",
      "loss=[153269.50450747]\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tx_poly.shape[1], 1))\n",
    "\n",
    "w, tr_loss = reg_logistic_regression(y_train, tx_train, initial_w, 0.001, 10000, 0.1, method='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68696"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tx_test)\n",
    "\n",
    "(y_pred == y_test).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Result with SGD, gamma = 0.1, degree 2, 10k iter: loss = 115658, test set accuracy = 0.68696\n",
    "- Result with Newton, gamma = 0.1, degree 2, 10k iter: loss = 126817, test set accuracy = 0.6554\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[139955.25046312]\n",
      "||d|| = 2.7131764979399953\n",
      "Current iteration=1000, loss=[129131.8432457]\n",
      "||d|| = 5.66480554398374\n",
      "Current iteration=2000, loss=[128452.4336647]\n",
      "||d|| = 3.358482380966959\n",
      "Current iteration=3000, loss=[128904.39914444]\n",
      "||d|| = 3.1543797487187564\n",
      "Current iteration=4000, loss=[128822.17365715]\n",
      "||d|| = 3.870159339983603\n",
      "Current iteration=5000, loss=[128707.81919923]\n",
      "||d|| = 2.157741942193498\n",
      "Current iteration=6000, loss=[128793.65809358]\n",
      "||d|| = 5.063492435556481\n",
      "Current iteration=7000, loss=[128838.25429689]\n",
      "||d|| = 2.116481931366886\n",
      "Current iteration=8000, loss=[128900.86620889]\n",
      "||d|| = 2.117993899172692\n",
      "Current iteration=9000, loss=[128713.22159776]\n",
      "||d|| = 2.10311382058259\n",
      "loss=[127524.0134424]\n",
      "lambda=1e-05, Training loss=128856.288, Testing loss=32318.584\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[141646.78530174]\n",
      "||d|| = 28.187607308561805\n",
      "Current iteration=1000, loss=[129927.61377292]\n",
      "||d|| = 2.4503415278703855\n",
      "Current iteration=2000, loss=[129955.37373749]\n",
      "||d|| = 16.759185115209764\n",
      "Current iteration=3000, loss=[129650.37548057]\n",
      "||d|| = 15.655089312718157\n",
      "Current iteration=4000, loss=[129785.45451969]\n",
      "||d|| = 15.901671251913625\n",
      "Current iteration=5000, loss=[129751.18795999]\n",
      "||d|| = 2.960402128659585\n",
      "Current iteration=6000, loss=[130031.33914559]\n",
      "||d|| = 3.1234916099546064\n",
      "Current iteration=7000, loss=[130065.11707873]\n",
      "||d|| = 3.182448257332641\n",
      "Current iteration=8000, loss=[130106.15652804]\n",
      "||d|| = 15.582948339156717\n",
      "Current iteration=9000, loss=[130300.83062946]\n",
      "||d|| = 71.34588967679093\n",
      "loss=[127446.04902751]\n",
      "lambda=2.2758459260747865e-05, Training loss=130464.672, Testing loss=32751.888\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[145496.45797302]\n",
      "||d|| = 2.531601252423577\n",
      "Current iteration=1000, loss=[135961.72006846]\n",
      "||d|| = 2.2491489849639956\n",
      "Current iteration=2000, loss=[135262.39969597]\n",
      "||d|| = 17.04478298630518\n",
      "Current iteration=3000, loss=[134629.85381974]\n",
      "||d|| = 17.000398862598132\n",
      "Current iteration=4000, loss=[134597.42473853]\n",
      "||d|| = 9.659519689431908\n",
      "Current iteration=5000, loss=[134142.27606445]\n",
      "||d|| = 2.098617240727721\n",
      "Current iteration=6000, loss=[133943.99611995]\n",
      "||d|| = 3.3207181441292417\n",
      "Current iteration=7000, loss=[133843.29143138]\n",
      "||d|| = 3.0122341740678484\n",
      "Current iteration=8000, loss=[134286.68063831]\n",
      "||d|| = 3.1047074104077357\n",
      "Current iteration=9000, loss=[134309.33305083]\n",
      "||d|| = 2.150694472922891\n",
      "loss=[127234.40136538]\n",
      "lambda=5.1794746792312125e-05, Training loss=134104.363, Testing loss=33727.445\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[154257.71983868]\n",
      "||d|| = 2.8235583435705425\n",
      "Current iteration=1000, loss=[143511.13278922]\n",
      "||d|| = 2.318591422602951\n",
      "Current iteration=2000, loss=[143374.5714952]\n",
      "||d|| = 7.2163015615268264\n",
      "Current iteration=3000, loss=[142974.05965127]\n",
      "||d|| = 16.072771188547804\n",
      "Current iteration=4000, loss=[142621.06839756]\n",
      "||d|| = 2.7262640481360054\n",
      "Current iteration=5000, loss=[142674.80110621]\n",
      "||d|| = 16.11527744693048\n",
      "Current iteration=6000, loss=[142682.23526049]\n",
      "||d|| = 4.513608085468858\n",
      "Current iteration=7000, loss=[142711.0062451]\n",
      "||d|| = 15.828198169218016\n",
      "Current iteration=8000, loss=[142866.89960979]\n",
      "||d|| = 15.658467900984968\n",
      "Current iteration=9000, loss=[142950.82089006]\n",
      "||d|| = 2.535128984523081\n",
      "loss=[127299.01931543]\n",
      "lambda=0.00011787686347935866, Training loss=142939.084, Testing loss=36092.490\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[174197.00196292]\n",
      "||d|| = 28.195522388880942\n",
      "Current iteration=1000, loss=[163987.47683102]\n",
      "||d|| = 3.360778352368673\n",
      "Current iteration=2000, loss=[164384.73956165]\n",
      "||d|| = 2.978331526102033\n",
      "Current iteration=3000, loss=[163990.32861992]\n",
      "||d|| = 2.138949343601188\n",
      "Current iteration=4000, loss=[163582.82269121]\n",
      "||d|| = 3.0064452207487364\n",
      "Current iteration=5000, loss=[163700.11932021]\n",
      "||d|| = 6.436336512607976\n",
      "Current iteration=6000, loss=[163387.91568769]\n",
      "||d|| = 7.895062916376463\n",
      "Current iteration=7000, loss=[163244.35687963]\n",
      "||d|| = 3.415178959493191\n",
      "Current iteration=8000, loss=[163090.36855506]\n",
      "||d|| = 3.345626184157206\n",
      "Current iteration=9000, loss=[162933.67582875]\n",
      "||d|| = 3.61536737075599\n",
      "loss=[127768.00899208]\n",
      "lambda=0.0002682695795279727, Training loss=163331.340, Testing loss=41553.186\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[219575.73595422]\n",
      "||d|| = 2.661379603033474\n",
      "Current iteration=1000, loss=[209392.57602096]\n",
      "||d|| = 2.1751328644119763\n",
      "Current iteration=2000, loss=[208941.29580196]\n",
      "||d|| = 12.896978726662867\n",
      "Current iteration=3000, loss=[208366.72434228]\n",
      "||d|| = 3.4325111172623988\n",
      "Current iteration=4000, loss=[208530.48464217]\n",
      "||d|| = 3.1754240630277164\n",
      "Current iteration=5000, loss=[208685.20780959]\n",
      "||d|| = 2.9736531321023976\n",
      "Current iteration=6000, loss=[208613.51220009]\n",
      "||d|| = 16.105198981874132\n",
      "Current iteration=7000, loss=[208121.57080953]\n",
      "||d|| = 2.2173009698257236\n",
      "Current iteration=8000, loss=[208137.99107842]\n",
      "||d|| = 2.3147126673968255\n",
      "Current iteration=9000, loss=[208311.24864369]\n",
      "||d|| = 2.138555191627609\n",
      "loss=[127320.49368544]\n",
      "lambda=0.0006105402296585327, Training loss=208265.847, Testing loss=53605.806\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[322850.74283876]\n",
      "||d|| = 2.640227287813802\n",
      "Current iteration=1000, loss=[312213.73256353]\n",
      "||d|| = 3.1994087763058583\n",
      "Current iteration=2000, loss=[311843.03321814]\n",
      "||d|| = 2.11019498985013\n",
      "Current iteration=3000, loss=[311373.77086799]\n",
      "||d|| = 40.67888602163192\n",
      "Current iteration=4000, loss=[310886.0808615]\n",
      "||d|| = 3.3405250444232086\n",
      "Current iteration=5000, loss=[310863.41473651]\n",
      "||d|| = 2.721713151913254\n",
      "Current iteration=6000, loss=[311003.80836217]\n",
      "||d|| = 4.908861620848498\n",
      "Current iteration=7000, loss=[310964.50532434]\n",
      "||d|| = 3.0417051456894106\n",
      "Current iteration=8000, loss=[311335.04403171]\n",
      "||d|| = 3.29749047933466\n",
      "Current iteration=9000, loss=[311360.80557244]\n",
      "||d|| = 2.477708565608854\n",
      "loss=[127165.66714912]\n",
      "lambda=0.0013894954943731374, Training loss=311386.749, Testing loss=81241.881\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[557888.74652228]\n",
      "||d|| = 3.141741270716292\n",
      "Current iteration=1000, loss=[546976.23340872]\n",
      "||d|| = 2.222886679277222\n",
      "Current iteration=2000, loss=[547099.20650191]\n",
      "||d|| = 2.134543622995221\n",
      "Current iteration=3000, loss=[547115.76110498]\n",
      "||d|| = 2.5889897538912034\n",
      "Current iteration=4000, loss=[547055.54452067]\n",
      "||d|| = 2.93331045843041\n",
      "Current iteration=5000, loss=[547015.11738666]\n",
      "||d|| = 2.1496146407469543\n",
      "Current iteration=6000, loss=[546857.96945031]\n",
      "||d|| = 16.17330238791353\n",
      "Current iteration=7000, loss=[546881.09412335]\n",
      "||d|| = 15.922266159710837\n",
      "Current iteration=8000, loss=[546972.45835623]\n",
      "||d|| = 2.9889978370068713\n",
      "Current iteration=9000, loss=[546948.4497043]\n",
      "||d|| = 17.124316897636604\n",
      "loss=[127608.49564979]\n",
      "lambda=0.0031622776601683794, Training loss=546864.646, Testing loss=144328.175\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[1092799.02967818]\n",
      "||d|| = 2.538883905875992\n",
      "Current iteration=1000, loss=[1082824.02411356]\n",
      "||d|| = 2.219687631642439\n",
      "Current iteration=2000, loss=[1082877.86712605]\n",
      "||d|| = 6.401111125294508\n",
      "Current iteration=3000, loss=[1083026.88360993]\n",
      "||d|| = 3.7162915872277\n",
      "Current iteration=4000, loss=[1082425.46325812]\n",
      "||d|| = 3.214832161487799\n",
      "Current iteration=5000, loss=[1082027.21322238]\n",
      "||d|| = 8.18298265485076\n",
      "Current iteration=6000, loss=[1082126.15272303]\n",
      "||d|| = 3.085579893556429\n",
      "Current iteration=7000, loss=[1082224.85499888]\n",
      "||d|| = 3.0869006643457064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1ecf0672f1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training accuracy={acc:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mpoly_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-1ecf0672f1c6>\u001b[0m in \u001b[0;36mpoly_reg_logistic_regression\u001b[0;34m(tx, y, degree, ratio)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mte_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documenti/EPFL/M1/ML/projects/ml_project_1/src/logistic/not_req_impl.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, initial_w, lambda_, max_iters, gamma, method, ratio)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documenti/EPFL/M1/ML/projects/ml_project_1/src/logistic/not_req_impl.py\u001b[0m in \u001b[0;36mgradient_descent_step\u001b[0;34m(y, tx, w, gamma, lambda_, method)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Get loss, gradient, hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mrand_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documenti/EPFL/M1/ML/projects/ml_project_1/src/logistic/loss.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msumming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0my_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msumming\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_component\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def poly_reg_logistic_regression(tx, y, degree, ratio):\n",
    "    \n",
    "    tx_poly = build_poly_matrix_vandermonde(tx, degree)\n",
    "    tx_train, y_train, tx_test, y_test = split_data(tx_poly, y, ratio)\n",
    " \n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    initial_w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "    ws = []\n",
    "    tr_losses = []\n",
    "    te_losses = []\n",
    "\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "\n",
    "\n",
    "        w, tr_loss = reg_logistic_regression(y_train, tx_train, initial_w, lambda_, 10000, 0.1, method='newton')\n",
    "        ws.append(w)\n",
    "        te_loss = compute_loss(y_test, tx_test, w, lambda_=lambda_)\n",
    "        tr_losses.append(tr_loss.flatten()[0])\n",
    "        te_losses.append(te_loss.flatten()[0])\n",
    "\n",
    "        y_pred = predict_labels(w, tx_test)\n",
    "        training_accuracy = (y_pred == y_test).sum() / y_test.shape[0]\n",
    "\n",
    "        print(\"lambda={lambda_}, Training loss={tr:.3f}, Testing loss={te:.3f}\".format(\n",
    "            lambda_=lambda_, tr=tr_losses[ind], te=te_losses[ind]))\n",
    "        print(\"Training accuracy={acc:.3f}\".format(acc=training_accuracy))\n",
    "\n",
    "poly_reg_logistic_regression(tx, y, 2, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub, x_sub_raw, ids_sub = load_csv_data('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub = flatten_jet_features(x_sub_raw)\n",
    "tx_sub = np.c_[np.ones((y_sub.shape[0], 1)), x_sub]\n",
    "tx_sub_poly = build_poly_matrix_vandermonde(tx_sub, 2)\n",
    "y_sub = predict_labels(w, tx_sub_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_sub, y_sub, '../submissions/10-24.14-18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
