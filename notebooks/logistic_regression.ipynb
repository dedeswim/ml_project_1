{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from src.helpers import load_csv_data, standardize, remove_incomplete_columns, predict_labels, create_csv_submission\n",
    "from src.logistic.implementations import logistic_regression\n",
    "from src.logistic.loss import compute_loss\n",
    "from src.logistic.not_req_impl import reg_logistic_regression, gradient_descent_step\n",
    "from src.logistic.gradient import compute_gradient\n",
    "from src.split import split_data\n",
    "\n",
    "from src.logistic.sigmoid import sigmoid\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "x, kept_columns = remove_incomplete_columns(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize x and add the 1s column\n",
    "x, mean_x, std_x = standardize(x)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "tx_train, y_train, tx_test, y_test = split_data(tx, y, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2262984\n",
      "Current iteration=73300, loss=[136962.1559508]\n",
      "||d|| = 0.5941439287913605\n",
      "Current iteration=73400, loss=[137321.24478922]\n",
      "||d|| = 1.3720806001791432\n",
      "Current iteration=73500, loss=[136879.39428901]\n",
      "||d|| = 0.21088987013687352\n",
      "Current iteration=73600, loss=[138473.90002365]\n",
      "||d|| = 0.34067314234831886\n",
      "Current iteration=73700, loss=[136274.09138344]\n",
      "||d|| = 0.897425031512749\n",
      "Current iteration=73800, loss=[136676.012739]\n",
      "||d|| = 0.3395781969356725\n",
      "Current iteration=73900, loss=[135257.75571366]\n",
      "||d|| = 0.5371458495169106\n",
      "Current iteration=74000, loss=[135827.87017012]\n",
      "||d|| = 0.5281365873490873\n",
      "Current iteration=74100, loss=[136897.46244334]\n",
      "||d|| = 0.5885020672193488\n",
      "Current iteration=74200, loss=[135893.05574127]\n",
      "||d|| = 0.4155854003366906\n",
      "Current iteration=74300, loss=[136641.44592865]\n",
      "||d|| = 1.22564450074405\n",
      "Current iteration=74400, loss=[135783.66237767]\n",
      "||d|| = 2.093968465640055\n",
      "Current iteration=74500, loss=[136596.64678365]\n",
      "||d|| = 2.1149871532145914\n",
      "Current iteration=74600, loss=[136258.84588905]\n",
      "||d|| = 0.8096951897669722\n",
      "Current iteration=74700, loss=[136736.49082646]\n",
      "||d|| = 0.8213712307306945\n",
      "Current iteration=74800, loss=[138063.42616]\n",
      "||d|| = 0.2278657338058619\n",
      "Current iteration=74900, loss=[137040.1980216]\n",
      "||d|| = 0.21842975001555287\n",
      "Current iteration=75000, loss=[137647.667955]\n",
      "||d|| = 1.3575652855039115\n",
      "Current iteration=75100, loss=[135710.61048851]\n",
      "||d|| = 0.30615904421757895\n",
      "Current iteration=75200, loss=[135558.86205317]\n",
      "||d|| = 1.943717973689379\n",
      "Current iteration=75300, loss=[135506.13478126]\n",
      "||d|| = 2.445567161100437\n",
      "Current iteration=75400, loss=[135860.29510536]\n",
      "||d|| = 0.5645488414034272\n",
      "Current iteration=75500, loss=[135325.64521849]\n",
      "||d|| = 0.6540026774783357\n",
      "Current iteration=75600, loss=[137197.77557128]\n",
      "||d|| = 0.6445516128958383\n",
      "Current iteration=75700, loss=[138231.71102307]\n",
      "||d|| = 0.012083243824925422\n",
      "Current iteration=75800, loss=[135727.24465127]\n",
      "||d|| = 0.5317010178386117\n",
      "Current iteration=75900, loss=[136441.81906201]\n",
      "||d|| = 0.3482561574259675\n",
      "Current iteration=76000, loss=[136119.8080316]\n",
      "||d|| = 0.6083588056733749\n",
      "Current iteration=76100, loss=[136926.55247296]\n",
      "||d|| = 0.42302177181305106\n",
      "Current iteration=76200, loss=[136121.85802918]\n",
      "||d|| = 0.7172885338508076\n",
      "Current iteration=76300, loss=[140165.48476815]\n",
      "||d|| = 0.4372332871321385\n",
      "Current iteration=76400, loss=[136435.87068562]\n",
      "||d|| = 0.9965604333646012\n",
      "Current iteration=76500, loss=[136263.36372573]\n",
      "||d|| = 0.21372553423371346\n",
      "Current iteration=76600, loss=[138328.79015843]\n",
      "||d|| = 0.13667412237125365\n",
      "Current iteration=76700, loss=[138224.41569547]\n",
      "||d|| = 1.4552384874457702\n",
      "Current iteration=76800, loss=[138611.51907609]\n",
      "||d|| = 2.3601664938601434\n",
      "Current iteration=76900, loss=[136858.94821414]\n",
      "||d|| = 0.16096119470313144\n",
      "Current iteration=77000, loss=[137464.91719354]\n",
      "||d|| = 0.11743227404055834\n",
      "Current iteration=77100, loss=[139018.62880727]\n",
      "||d|| = 3.0333207710700894\n",
      "Current iteration=77200, loss=[137183.50570746]\n",
      "||d|| = 0.37860686063303234\n",
      "Current iteration=77300, loss=[137079.00207914]\n",
      "||d|| = 0.8223199399895709\n",
      "Current iteration=77400, loss=[135388.49182474]\n",
      "||d|| = 0.4568285749134572\n",
      "Current iteration=77500, loss=[138199.60593426]\n",
      "||d|| = 1.0456215923755237\n",
      "Current iteration=77600, loss=[136083.84855041]\n",
      "||d|| = 0.27680120229674865\n",
      "Current iteration=77700, loss=[140515.396451]\n",
      "||d|| = 2.227404116501854\n",
      "Current iteration=77800, loss=[135605.34028311]\n",
      "||d|| = 0.5666352344010532\n",
      "Current iteration=77900, loss=[136022.65017418]\n",
      "||d|| = 0.7590624714417321\n",
      "Current iteration=78000, loss=[135629.91669508]\n",
      "||d|| = 0.558183392939043\n",
      "Current iteration=78100, loss=[135876.55618355]\n",
      "||d|| = 0.7846813638824649\n",
      "Current iteration=78200, loss=[135386.08275021]\n",
      "||d|| = 0.4192032959487607\n",
      "Current iteration=78300, loss=[135599.19737364]\n",
      "||d|| = 0.36409313358164636\n",
      "Current iteration=78400, loss=[138459.4622301]\n",
      "||d|| = 0.503560833939841\n",
      "Current iteration=78500, loss=[138802.90333841]\n",
      "||d|| = 0.20763963529572158\n",
      "Current iteration=78600, loss=[137280.37166838]\n",
      "||d|| = 0.8316966780767842\n",
      "Current iteration=78700, loss=[137030.34166475]\n",
      "||d|| = 0.5608940810181343\n",
      "Current iteration=78800, loss=[137165.0790754]\n",
      "||d|| = 2.194633755515415\n",
      "Current iteration=78900, loss=[135930.81709237]\n",
      "||d|| = 0.7266558743130714\n",
      "Current iteration=79000, loss=[135348.37960201]\n",
      "||d|| = 1.478565566966212\n",
      "Current iteration=79100, loss=[135325.49346795]\n",
      "||d|| = 1.382746100686768\n",
      "Current iteration=79200, loss=[135690.47495297]\n",
      "||d|| = 0.595114442649145\n",
      "Current iteration=79300, loss=[135474.54852514]\n",
      "||d|| = 0.8594347206345317\n",
      "Current iteration=79400, loss=[135517.74723766]\n",
      "||d|| = 1.160050177943502\n",
      "Current iteration=79500, loss=[137388.72167416]\n",
      "||d|| = 1.9377815149268758\n",
      "Current iteration=79600, loss=[138167.92661945]\n",
      "||d|| = 4.320968673512439\n",
      "Current iteration=79700, loss=[136630.25105502]\n",
      "||d|| = 0.7122501215286166\n",
      "Current iteration=79800, loss=[135746.36637874]\n",
      "||d|| = 0.9446040219946982\n",
      "Current iteration=79900, loss=[138565.26158933]\n",
      "||d|| = 0.7668555699331261\n",
      "Current iteration=80000, loss=[135322.44072983]\n",
      "||d|| = 1.8065474982883278\n",
      "Current iteration=80100, loss=[135586.98055432]\n",
      "||d|| = 0.9600604138617109\n",
      "Current iteration=80200, loss=[137757.35632777]\n",
      "||d|| = 0.7565640571487003\n",
      "Current iteration=80300, loss=[136554.31468226]\n",
      "||d|| = 0.7918219450590238\n",
      "Current iteration=80400, loss=[135831.18572602]\n",
      "||d|| = 0.668342646195651\n",
      "Current iteration=80500, loss=[136924.56604436]\n",
      "||d|| = 0.7835311433065989\n",
      "Current iteration=80600, loss=[135624.30258271]\n",
      "||d|| = 0.5709179061846746\n",
      "Current iteration=80700, loss=[137381.4887258]\n",
      "||d|| = 1.5733080380066955\n",
      "Current iteration=80800, loss=[136695.27012067]\n",
      "||d|| = 0.738963981992958\n",
      "Current iteration=80900, loss=[138135.09515519]\n",
      "||d|| = 1.4216497170456814\n",
      "Current iteration=81000, loss=[136987.97734809]\n",
      "||d|| = 1.032014474495985\n",
      "Current iteration=81100, loss=[136817.55906942]\n",
      "||d|| = 1.514787742694094\n",
      "Current iteration=81200, loss=[139108.57144215]\n",
      "||d|| = 1.1932829152753766\n",
      "Current iteration=81300, loss=[137042.56108166]\n",
      "||d|| = 0.6774602162870101\n",
      "Current iteration=81400, loss=[141016.73776211]\n",
      "||d|| = 0.901497828809325\n",
      "Current iteration=81500, loss=[137848.5480855]\n",
      "||d|| = 0.5631915323602636\n",
      "Current iteration=81600, loss=[136102.12084586]\n",
      "||d|| = 0.8530139474753351\n",
      "Current iteration=81700, loss=[135991.7613623]\n",
      "||d|| = 0.762671903929233\n",
      "Current iteration=81800, loss=[141891.93915023]\n",
      "||d|| = 0.26714281535570367\n",
      "Current iteration=81900, loss=[136123.56195912]\n",
      "||d|| = 2.1722994214962097\n",
      "Current iteration=82000, loss=[137147.8659869]\n",
      "||d|| = 1.553957470764361\n",
      "Current iteration=82100, loss=[135939.89340467]\n",
      "||d|| = 0.8078795640367309\n",
      "Current iteration=82200, loss=[136619.43837408]\n",
      "||d|| = 1.6191994681166229\n",
      "Current iteration=82300, loss=[137953.36991115]\n",
      "||d|| = 1.708257839829505\n",
      "Current iteration=82400, loss=[135901.19637323]\n",
      "||d|| = 0.6711019688808191\n",
      "Current iteration=82500, loss=[136181.08466817]\n",
      "||d|| = 0.5445134034048231\n",
      "Current iteration=82600, loss=[136489.95760112]\n",
      "||d|| = 0.8891447003221056\n",
      "Current iteration=82700, loss=[136486.71530182]\n",
      "||d|| = 0.5819801914857133\n",
      "Current iteration=82800, loss=[137756.26515111]\n",
      "||d|| = 0.7168449610819977\n",
      "Current iteration=82900, loss=[136050.72729676]\n",
      "||d|| = 0.48845798445542393\n",
      "Current iteration=83000, loss=[135888.66478226]\n",
      "||d|| = 1.5618043548582774\n",
      "Current iteration=83100, loss=[136987.26628544]\n",
      "||d|| = 0.44575810031479324\n",
      "Current iteration=83200, loss=[136487.33136321]\n",
      "||d|| = 0.9058371113285273\n",
      "Current iteration=83300, loss=[145975.27686118]\n",
      "||d|| = 0.985795496757725\n",
      "Current iteration=83400, loss=[135642.44656744]\n",
      "||d|| = 0.7271016674480723\n",
      "Current iteration=83500, loss=[137753.81663614]\n",
      "||d|| = 0.6585128362670096\n",
      "Current iteration=83600, loss=[137280.55896394]\n",
      "||d|| = 1.7724284347657822\n",
      "Current iteration=83700, loss=[135381.5798417]\n",
      "||d|| = 0.26567448230915736\n",
      "Current iteration=83800, loss=[135511.23306983]\n",
      "||d|| = 0.6594024827369679\n",
      "Current iteration=83900, loss=[136597.04691229]\n",
      "||d|| = 1.5698580735953946\n",
      "Current iteration=84000, loss=[135402.99106222]\n",
      "||d|| = 0.4114875047042541\n",
      "Current iteration=84100, loss=[136169.09248219]\n",
      "||d|| = 1.0134516788997452\n",
      "Current iteration=84200, loss=[135368.28890862]\n",
      "||d|| = 0.6792565148272959\n",
      "Current iteration=84300, loss=[135976.6677374]\n",
      "||d|| = 0.49749202627184463\n",
      "Current iteration=84400, loss=[136388.37055352]\n",
      "||d|| = 0.5703272688444369\n",
      "Current iteration=84500, loss=[138033.57228926]\n",
      "||d|| = 0.8280458093810017\n",
      "Current iteration=84600, loss=[143091.54576824]\n",
      "||d|| = 0.9587804822467353\n",
      "Current iteration=84700, loss=[136242.81649109]\n",
      "||d|| = 2.4872526737888787\n",
      "Current iteration=84800, loss=[136326.73767511]\n",
      "||d|| = 1.7019054179531636\n",
      "Current iteration=84900, loss=[135612.32320111]\n",
      "||d|| = 2.484874328043018\n",
      "Current iteration=85000, loss=[135915.79817326]\n",
      "||d|| = 1.8458262753296304\n",
      "Current iteration=85100, loss=[135852.90009831]\n",
      "||d|| = 2.781380848731946\n",
      "Current iteration=85200, loss=[136072.22183125]\n",
      "||d|| = 0.7647750745711382\n",
      "Current iteration=85300, loss=[138913.81872394]\n",
      "||d|| = 2.72799735518996\n",
      "Current iteration=85400, loss=[135238.19315814]\n",
      "||d|| = 1.010378770195894\n",
      "Current iteration=85500, loss=[137829.64690962]\n",
      "||d|| = 0.7732941997199992\n",
      "Current iteration=85600, loss=[139043.74909957]\n",
      "||d|| = 0.5832617416906453\n",
      "Current iteration=85700, loss=[136251.94709118]\n",
      "||d|| = 0.3515536819322966\n",
      "Current iteration=85800, loss=[136893.3414065]\n",
      "||d|| = 8.210937806679544\n",
      "Current iteration=85900, loss=[135545.07809101]\n",
      "||d|| = 0.49459288474788227\n",
      "Current iteration=86000, loss=[135883.10482963]\n",
      "||d|| = 0.3689721894246625\n",
      "Current iteration=86100, loss=[135367.09190683]\n",
      "||d|| = 1.4039206338859398\n",
      "Current iteration=86200, loss=[135878.3795383]\n",
      "||d|| = 0.5239626391219737\n",
      "Current iteration=86300, loss=[135879.34236437]\n",
      "||d|| = 2.305034299571613\n",
      "Current iteration=86400, loss=[136634.01348748]\n",
      "||d|| = 2.062974527611207\n",
      "Current iteration=86500, loss=[136514.03632238]\n",
      "||d|| = 0.4568062818156287\n",
      "Current iteration=86600, loss=[136125.72975188]\n",
      "||d|| = 3.2565243625589892\n",
      "Current iteration=86700, loss=[135388.36301794]\n",
      "||d|| = 2.295902941726232\n",
      "Current iteration=86800, loss=[135452.73492512]\n",
      "||d|| = 0.7538741406477719\n",
      "Current iteration=86900, loss=[135452.28076288]\n",
      "||d|| = 0.5058779040318773\n",
      "Current iteration=87000, loss=[136504.76813101]\n",
      "||d|| = 0.45367959939166197\n",
      "Current iteration=87100, loss=[135890.06717403]\n",
      "||d|| = 0.2725773674929108\n",
      "Current iteration=87200, loss=[137097.84366951]\n",
      "||d|| = 0.6476524969777108\n",
      "Current iteration=87300, loss=[135685.83825204]\n",
      "||d|| = 1.1334568270122116\n",
      "Current iteration=87400, loss=[135959.12064052]\n",
      "||d|| = 0.3793838748341109\n",
      "Current iteration=87500, loss=[135897.42356261]\n",
      "||d|| = 1.4819397272827186\n",
      "Current iteration=87600, loss=[140495.4248323]\n",
      "||d|| = 1.4033741416996794\n",
      "Current iteration=87700, loss=[136052.90365081]\n",
      "||d|| = 2.375246869515438\n",
      "Current iteration=87800, loss=[135758.46643793]\n",
      "||d|| = 0.7259245801913019\n",
      "Current iteration=87900, loss=[135928.37028052]\n",
      "||d|| = 3.106289588725011\n",
      "Current iteration=88000, loss=[135973.53670923]\n",
      "||d|| = 0.5178693952273464\n",
      "Current iteration=88100, loss=[136722.22104877]\n",
      "||d|| = 1.2599604755721134\n",
      "Current iteration=88200, loss=[137016.14845075]\n",
      "||d|| = 9.025863783444619\n",
      "Current iteration=88300, loss=[141354.67380824]\n",
      "||d|| = 0.8532531687090217\n",
      "Current iteration=88400, loss=[135592.06179946]\n",
      "||d|| = 0.547005035459587\n",
      "Current iteration=88500, loss=[137336.86426319]\n",
      "||d|| = 9.132729233458427\n",
      "Current iteration=88600, loss=[136531.71393369]\n",
      "||d|| = 0.7379708571460744\n",
      "Current iteration=88700, loss=[136773.73584981]\n",
      "||d|| = 1.0826087863933622\n",
      "Current iteration=88800, loss=[136774.30482823]\n",
      "||d|| = 1.1216960498344188\n",
      "Current iteration=88900, loss=[135891.10654497]\n",
      "||d|| = 0.3566391199543118\n",
      "Current iteration=89000, loss=[135300.42657569]\n",
      "||d|| = 0.4752540021182206\n",
      "Current iteration=89100, loss=[135838.93243484]\n",
      "||d|| = 0.7668563116975473\n",
      "Current iteration=89200, loss=[138615.36615186]\n",
      "||d|| = 0.6020275237192694\n",
      "Current iteration=89300, loss=[135720.67149391]\n",
      "||d|| = 2.801332472873333\n",
      "Current iteration=89400, loss=[136113.55145776]\n",
      "||d|| = 1.0835750709972074\n",
      "Current iteration=89500, loss=[137320.72262695]\n",
      "||d|| = 1.9643478690259886\n",
      "Current iteration=89600, loss=[137847.43964142]\n",
      "||d|| = 0.10922238971917955\n",
      "Current iteration=89700, loss=[137532.26573446]\n",
      "||d|| = 0.27323289177613497\n",
      "Current iteration=89800, loss=[140793.96546135]\n",
      "||d|| = 0.49166761711440055\n",
      "Current iteration=89900, loss=[136878.4532025]\n",
      "||d|| = 0.5070532913159556\n",
      "Current iteration=90000, loss=[137573.07102101]\n",
      "||d|| = 0.24733849915656117\n",
      "Current iteration=90100, loss=[136461.94474493]\n",
      "||d|| = 1.5185950258668335\n",
      "Current iteration=90200, loss=[136313.86011641]\n",
      "||d|| = 2.0809733111021935\n",
      "Current iteration=90300, loss=[136150.26190789]\n",
      "||d|| = 1.95824983915048\n",
      "Current iteration=90400, loss=[137216.38258127]\n",
      "||d|| = 0.8219184200143094\n",
      "Current iteration=90500, loss=[137818.42946337]\n",
      "||d|| = 1.466383091974109\n",
      "Current iteration=90600, loss=[143063.23474744]\n",
      "||d|| = 1.786477037842128\n",
      "Current iteration=90700, loss=[135954.00193905]\n",
      "||d|| = 0.8723906525974962\n",
      "Current iteration=90800, loss=[135220.06548456]\n",
      "||d|| = 0.8332312913966156\n",
      "Current iteration=90900, loss=[135590.80948095]\n",
      "||d|| = 0.6666724972394255\n",
      "Current iteration=91000, loss=[136471.12651422]\n",
      "||d|| = 0.9779528615746541\n",
      "Current iteration=91100, loss=[136139.87655462]\n",
      "||d|| = 0.278202400268353\n",
      "Current iteration=91200, loss=[136576.70911024]\n",
      "||d|| = 1.6743626319567664\n",
      "Current iteration=91300, loss=[136047.39459987]\n",
      "||d|| = 0.8986778093403642\n",
      "Current iteration=91400, loss=[135342.15687763]\n",
      "||d|| = 0.31315485153065614\n",
      "Current iteration=91500, loss=[135846.00714128]\n",
      "||d|| = 2.3931719742069\n",
      "Current iteration=91600, loss=[137878.31276662]\n",
      "||d|| = 0.8522207412726946\n",
      "Current iteration=91700, loss=[136244.34569071]\n",
      "||d|| = 0.6308070019470555\n",
      "Current iteration=91800, loss=[135205.26047855]\n",
      "||d|| = 0.4075997616254629\n",
      "Current iteration=91900, loss=[137308.5277344]\n",
      "||d|| = 0.9034454255774695\n",
      "Current iteration=92000, loss=[135960.21480356]\n",
      "||d|| = 1.1038992384068662\n",
      "Current iteration=92100, loss=[135434.36942563]\n",
      "||d|| = 0.8735270860885093\n",
      "Current iteration=92200, loss=[138425.52065935]\n",
      "||d|| = 0.5454950966506854\n",
      "Current iteration=92300, loss=[138096.43205155]\n",
      "||d|| = 0.7434759224123049\n",
      "Current iteration=92400, loss=[135839.98131473]\n",
      "||d|| = 0.2615064914821458\n",
      "Current iteration=92500, loss=[135456.91570905]\n",
      "||d|| = 0.8521432911301143\n",
      "Current iteration=92600, loss=[138101.58662391]\n",
      "||d|| = 0.47644156260171555\n",
      "Current iteration=92700, loss=[138657.96891294]\n",
      "||d|| = 1.487397793898417\n",
      "Current iteration=92800, loss=[136079.54837289]\n",
      "||d|| = 0.26207258488115887\n",
      "Current iteration=92900, loss=[135387.46619488]\n",
      "||d|| = 1.3207767813353346\n",
      "Current iteration=93000, loss=[135392.27546728]\n",
      "||d|| = 0.4026690772261116\n",
      "Current iteration=93100, loss=[136230.0925157]\n",
      "||d|| = 0.874673950262353\n",
      "Current iteration=93200, loss=[135695.40869822]\n",
      "||d|| = 0.48879663863583134\n",
      "Current iteration=93300, loss=[135952.30854697]\n",
      "||d|| = 0.782870444727551\n",
      "Current iteration=93400, loss=[135959.6475269]\n",
      "||d|| = 3.798187675906667\n",
      "Current iteration=93500, loss=[138895.49187024]\n",
      "||d|| = 2.1982934534491045\n",
      "Current iteration=93600, loss=[136465.53530516]\n",
      "||d|| = 1.9686435977451318\n",
      "Current iteration=93700, loss=[139166.72026377]\n",
      "||d|| = 0.8307481768590355\n",
      "Current iteration=93800, loss=[135715.21134043]\n",
      "||d|| = 1.912731956818151\n",
      "Current iteration=93900, loss=[136726.12708343]\n",
      "||d|| = 2.4740954414081506\n",
      "Current iteration=94000, loss=[136000.58176325]\n",
      "||d|| = 0.43853481423944507\n",
      "Current iteration=94100, loss=[135575.9806298]\n",
      "||d|| = 0.7992972034647281\n",
      "Current iteration=94200, loss=[137018.81872156]\n",
      "||d|| = 0.6917426284996631\n",
      "Current iteration=94300, loss=[138416.40265402]\n",
      "||d|| = 0.4086325275939456\n",
      "Current iteration=94400, loss=[137418.31863686]\n",
      "||d|| = 0.6743997643219525\n",
      "Current iteration=94500, loss=[136299.74384336]\n",
      "||d|| = 1.528367547436564\n",
      "Current iteration=94600, loss=[135522.72795542]\n",
      "||d|| = 1.7221858221966695\n",
      "Current iteration=94700, loss=[138751.8248946]\n",
      "||d|| = 1.3097015459825294\n",
      "Current iteration=94800, loss=[135953.13104509]\n",
      "||d|| = 1.1145826212992234\n",
      "Current iteration=94900, loss=[135527.88333417]\n",
      "||d|| = 1.6448457271600259\n",
      "Current iteration=95000, loss=[135811.53389132]\n",
      "||d|| = 1.2179612278127352\n",
      "Current iteration=95100, loss=[136185.39875861]\n",
      "||d|| = 0.7492930390815098\n",
      "Current iteration=95200, loss=[136933.11762018]\n",
      "||d|| = 1.3157693917266005\n",
      "Current iteration=95300, loss=[136459.5023392]\n",
      "||d|| = 0.5109633797749752\n",
      "Current iteration=95400, loss=[136525.06677506]\n",
      "||d|| = 1.3754562999650022\n",
      "Current iteration=95500, loss=[138841.221715]\n",
      "||d|| = 1.4917640422196945\n",
      "Current iteration=95600, loss=[138791.11344752]\n",
      "||d|| = 0.34879478609558984\n",
      "Current iteration=95700, loss=[137324.27937531]\n",
      "||d|| = 5.637589932555837\n",
      "Current iteration=95800, loss=[139914.66339287]\n",
      "||d|| = 1.8310731508785585\n",
      "Current iteration=95900, loss=[136027.94406438]\n",
      "||d|| = 0.49809674980115987\n",
      "Current iteration=96000, loss=[137441.56266682]\n",
      "||d|| = 0.5594748529740996\n",
      "Current iteration=96100, loss=[136607.50865003]\n",
      "||d|| = 0.4518622484091428\n",
      "Current iteration=96200, loss=[137081.91146637]\n",
      "||d|| = 1.1921464942261852\n",
      "Current iteration=96300, loss=[136100.09885187]\n",
      "||d|| = 0.34886575889359733\n",
      "Current iteration=96400, loss=[136077.65373597]\n",
      "||d|| = 1.1819066031911594\n",
      "Current iteration=96500, loss=[141404.37496489]\n",
      "||d|| = 1.7284622508767122\n",
      "Current iteration=96600, loss=[139494.15188337]\n",
      "||d|| = 0.9988616770581636\n",
      "Current iteration=96700, loss=[136318.44355727]\n",
      "||d|| = 2.8825072166409607\n",
      "Current iteration=96800, loss=[136512.00836419]\n",
      "||d|| = 0.9839163364128259\n",
      "Current iteration=96900, loss=[137762.79868872]\n",
      "||d|| = 0.6052142795595812\n",
      "Current iteration=97000, loss=[138183.19248164]\n",
      "||d|| = 0.5111657984546083\n",
      "Current iteration=97100, loss=[136752.80255875]\n",
      "||d|| = 1.3728021837139572\n",
      "Current iteration=97200, loss=[135463.49929572]\n",
      "||d|| = 0.390458898022712\n",
      "Current iteration=97300, loss=[135832.47208848]\n",
      "||d|| = 0.40662357845744584\n",
      "Current iteration=97400, loss=[136276.79286005]\n",
      "||d|| = 0.9248428040760083\n",
      "Current iteration=97500, loss=[135746.40921171]\n",
      "||d|| = 0.41724277052652664\n",
      "Current iteration=97600, loss=[136230.73283544]\n",
      "||d|| = 1.102091796233142\n",
      "Current iteration=97700, loss=[135934.25235885]\n",
      "||d|| = 0.06837299160383595\n",
      "Current iteration=97800, loss=[136191.66184951]\n",
      "||d|| = 10.978974246583114\n",
      "Current iteration=97900, loss=[137616.94218918]\n",
      "||d|| = 0.5325885470161582\n",
      "Current iteration=98000, loss=[135525.10788946]\n",
      "||d|| = 0.8360467771890168\n",
      "Current iteration=98100, loss=[135272.90307599]\n",
      "||d|| = 0.31580028128980536\n",
      "Current iteration=98200, loss=[136134.40568157]\n",
      "||d|| = 1.6195724834595047\n",
      "Current iteration=98300, loss=[137450.23424815]\n",
      "||d|| = 0.7376984821041233\n",
      "Current iteration=98400, loss=[138781.9547896]\n",
      "||d|| = 0.1295744692646685\n",
      "Current iteration=98500, loss=[136573.4053543]\n",
      "||d|| = 0.13471029046393262\n",
      "Current iteration=98600, loss=[140943.14094925]\n",
      "||d|| = 2.613680407549485\n",
      "Current iteration=98700, loss=[136512.97531205]\n",
      "||d|| = 1.0453742982522054\n",
      "Current iteration=98800, loss=[136957.3236311]\n",
      "||d|| = 0.2954618339737672\n",
      "Current iteration=98900, loss=[135997.63427794]\n",
      "||d|| = 0.1729234243944602\n",
      "Current iteration=99000, loss=[137407.733458]\n",
      "||d|| = 0.7299091858701512\n",
      "Current iteration=99100, loss=[135293.47889771]\n",
      "||d|| = 1.0842824544210283\n",
      "Current iteration=99200, loss=[135327.44714217]\n",
      "||d|| = 0.5526664411429096\n",
      "Current iteration=99300, loss=[135322.49052581]\n",
      "||d|| = 6.284650932613694\n",
      "Current iteration=99400, loss=[136380.78790787]\n",
      "||d|| = 1.1713439096176994\n",
      "Current iteration=99500, loss=[137049.09820429]\n",
      "||d|| = 0.9215059846555428\n",
      "Current iteration=99600, loss=[136358.67691208]\n",
      "||d|| = 0.30611853884594864\n",
      "Current iteration=99700, loss=[135947.79379875]\n",
      "||d|| = 0.5730456404417054\n",
      "Current iteration=99800, loss=[136453.17732375]\n",
      "||d|| = 1.7460546911303811\n",
      "Current iteration=99900, loss=[136686.55463375]\n",
      "||d|| = 0.44937032966833224\n",
      "loss=[138897.157523]\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "w, tr_loss = reg_logistic_regression(y, tx, initial_w, 0, 100000, 0.01, method='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result with gamma = 0.001 and 100k iter ||d|| = 0.8317845119149855 loss = 137795.11103903, test set accuracy = 0.67216\n",
    "\n",
    "Result with gamma = 0.01 and 100k iter ||d|| = 0.44937032966833224 loss = [138897.157523], test set accuracy = 0.67562\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub, x_sub_raw, ids_sub = load_csv_data('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub = x_sub_raw[:, kept_columns]\n",
    "tx_sub = np.c_[np.ones((y_sub.shape[0], 1)), x_sub]\n",
    "y_sub = predict_labels(w, tx_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_sub, y_sub, 'submissions/10-24.00-15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67562"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tx_test)\n",
    "\n",
    "(y_pred == y_test).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[173313.04513999]\n",
      "||d|| = 1.26305514443864\n",
      "Current iteration=1000, loss=[168283.06486387]\n",
      "||d|| = 1.819023295795367\n",
      "Current iteration=2000, loss=[166816.44837511]\n",
      "||d|| = 1.0077769045239235\n",
      "Current iteration=3000, loss=[166393.08782902]\n",
      "||d|| = 0.8318294301989898\n",
      "Current iteration=4000, loss=[166597.5718525]\n",
      "||d|| = 2.6272222280554387\n",
      "Current iteration=5000, loss=[166996.0668856]\n",
      "||d|| = 0.8493192680566024\n",
      "Current iteration=6000, loss=[166730.77618249]\n",
      "||d|| = 1.0512566439418087\n",
      "Current iteration=7000, loss=[166921.00926433]\n",
      "||d|| = 3.5558377129950416\n",
      "Current iteration=8000, loss=[166223.62885936]\n",
      "||d|| = 0.6963285060093969\n",
      "Current iteration=9000, loss=[167875.55098277]\n",
      "||d|| = 0.9897859278051709\n",
      "loss=[167745.57591656]\n",
      "lambda=1e-05, Training loss=167800.920, Testing loss=33595.736\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[173346.53609555]\n",
      "||d|| = 0.8728545565961453\n",
      "Current iteration=1000, loss=[168460.05859765]\n",
      "||d|| = 1.3735031108323827\n",
      "Current iteration=2000, loss=[167069.18261408]\n",
      "||d|| = 0.9031784225640527\n",
      "Current iteration=3000, loss=[166427.5666778]\n",
      "||d|| = 3.313056207041964\n",
      "Current iteration=4000, loss=[167170.58652437]\n",
      "||d|| = 0.8834238984740492\n",
      "Current iteration=5000, loss=[167043.85770693]\n",
      "||d|| = 1.2473836233963607\n",
      "Current iteration=6000, loss=[166729.30488526]\n",
      "||d|| = 1.2159318094135076\n",
      "Current iteration=7000, loss=[166632.11606819]\n",
      "||d|| = 0.8832334599975862\n",
      "Current iteration=8000, loss=[166763.64512231]\n",
      "||d|| = 4.1445262831747955\n",
      "Current iteration=9000, loss=[166562.0793495]\n",
      "||d|| = 1.1628000030728962\n",
      "loss=[166020.82586707]\n",
      "lambda=2.2758459260747865e-05, Training loss=166094.293, Testing loss=33251.834\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[173422.75635032]\n",
      "||d|| = 1.2541671263393699\n",
      "Current iteration=1000, loss=[168267.75156578]\n",
      "||d|| = 1.098835951073333\n",
      "Current iteration=2000, loss=[167396.42561941]\n",
      "||d|| = 0.9411108872373528\n",
      "Current iteration=3000, loss=[167039.85050206]\n",
      "||d|| = 2.1436378435095915\n",
      "Current iteration=4000, loss=[166872.30728394]\n",
      "||d|| = 1.9646701997822797\n",
      "Current iteration=5000, loss=[166646.85076211]\n",
      "||d|| = 1.3962040049126478\n",
      "Current iteration=6000, loss=[166654.94139525]\n",
      "||d|| = 1.6076990084711322\n",
      "Current iteration=7000, loss=[167043.47068016]\n",
      "||d|| = 1.128884724565772\n",
      "Current iteration=8000, loss=[165904.80905641]\n",
      "||d|| = 0.7805806719796041\n",
      "Current iteration=9000, loss=[166738.0094186]\n",
      "||d|| = 0.699603365068777\n",
      "loss=[167330.70533768]\n",
      "lambda=5.1794746792312125e-05, Training loss=167494.327, Testing loss=33535.242\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[173596.22190662]\n",
      "||d|| = 4.468479740656699\n",
      "Current iteration=1000, loss=[168780.10935006]\n",
      "||d|| = 1.0590788365502513\n",
      "Current iteration=2000, loss=[167488.76989055]\n",
      "||d|| = 0.8134251827799253\n",
      "Current iteration=3000, loss=[168279.21282592]\n",
      "||d|| = 2.7287072606298275\n",
      "Current iteration=4000, loss=[166998.47209364]\n",
      "||d|| = 4.160577533201783\n",
      "Current iteration=5000, loss=[166557.48707305]\n",
      "||d|| = 1.7463836256739027\n",
      "Current iteration=6000, loss=[167392.6339087]\n",
      "||d|| = 1.0156078423838548\n",
      "Current iteration=7000, loss=[167028.47444461]\n",
      "||d|| = 0.9517221485155577\n",
      "Current iteration=8000, loss=[167564.84988391]\n",
      "||d|| = 1.1450875966127358\n",
      "Current iteration=9000, loss=[167305.20664103]\n",
      "||d|| = 4.946945694418915\n",
      "loss=[166116.06750867]\n",
      "lambda=0.00011787686347935866, Training loss=166422.279, Testing loss=33322.213\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[173991.00278625]\n",
      "||d|| = 5.541875001276259\n",
      "Current iteration=1000, loss=[169008.12180402]\n",
      "||d|| = 1.1894661570349476\n",
      "Current iteration=2000, loss=[167900.61610083]\n",
      "||d|| = 0.9691297314027508\n",
      "Current iteration=3000, loss=[167722.1255699]\n",
      "||d|| = 3.941106161573914\n",
      "Current iteration=4000, loss=[168020.48954807]\n",
      "||d|| = 3.9783988896688998\n",
      "Current iteration=5000, loss=[168267.76251402]\n",
      "||d|| = 3.8984447531600726\n",
      "Current iteration=6000, loss=[166833.91175431]\n",
      "||d|| = 1.551085912807372\n",
      "Current iteration=7000, loss=[167967.10863758]\n",
      "||d|| = 4.230625059296272\n",
      "Current iteration=8000, loss=[168532.85373105]\n",
      "||d|| = 1.2087664063246752\n",
      "Current iteration=9000, loss=[167965.28337612]\n",
      "||d|| = 4.1658471614579975\n",
      "loss=[167316.34776422]\n",
      "lambda=0.0002682695795279727, Training loss=168049.173, Testing loss=33650.404\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[174889.46324284]\n",
      "||d|| = 1.3075116123363506\n",
      "Current iteration=1000, loss=[170178.57839484]\n",
      "||d|| = 1.0659202815722766\n",
      "Current iteration=2000, loss=[168277.63389409]\n",
      "||d|| = 4.0665948861999865\n",
      "Current iteration=3000, loss=[168142.08464237]\n",
      "||d|| = 0.8439289285317253\n",
      "Current iteration=4000, loss=[168179.26136932]\n",
      "||d|| = 0.6996768719667973\n",
      "Current iteration=5000, loss=[168023.42632725]\n",
      "||d|| = 0.742234542551109\n",
      "Current iteration=6000, loss=[168224.01697293]\n",
      "||d|| = 4.0098913250389865\n",
      "Current iteration=7000, loss=[168792.20399253]\n",
      "||d|| = 1.8220175592654464\n",
      "Current iteration=8000, loss=[168086.55194763]\n",
      "||d|| = 0.8435099568052427\n",
      "Current iteration=9000, loss=[168471.13327189]\n",
      "||d|| = 1.4574324534488317\n",
      "loss=[166596.63555184]\n",
      "lambda=0.0006105402296585327, Training loss=168216.929, Testing loss=33681.944\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[176934.22081272]\n",
      "||d|| = 1.0745192705562565\n",
      "Current iteration=1000, loss=[172029.98112241]\n",
      "||d|| = 2.305150031920144\n",
      "Current iteration=2000, loss=[171198.02330418]\n",
      "||d|| = 0.8313862483875408\n",
      "Current iteration=3000, loss=[170990.42928095]\n",
      "||d|| = 3.5123794213133843\n",
      "Current iteration=4000, loss=[171097.62788377]\n",
      "||d|| = 4.254099035527979\n",
      "Current iteration=5000, loss=[170594.07661055]\n",
      "||d|| = 1.7702771437202347\n",
      "Current iteration=6000, loss=[169714.25080376]\n",
      "||d|| = 1.349970127036815\n",
      "Current iteration=7000, loss=[170875.92250961]\n",
      "||d|| = 1.3530092364151882\n",
      "Current iteration=8000, loss=[171135.22269014]\n",
      "||d|| = 1.4027793649586415\n",
      "Current iteration=9000, loss=[171030.73494525]\n",
      "||d|| = 1.2807313141663457\n",
      "loss=[166835.02683782]\n",
      "lambda=0.0013894954943731374, Training loss=170502.150, Testing loss=34142.278\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[181587.77399793]\n",
      "||d|| = 1.1391853132546674\n",
      "Current iteration=1000, loss=[176157.46591759]\n",
      "||d|| = 3.0654332422329773\n",
      "Current iteration=2000, loss=[175163.43294165]\n",
      "||d|| = 3.4500489559902254\n",
      "Current iteration=3000, loss=[174506.44090795]\n",
      "||d|| = 1.0919129465111044\n",
      "Current iteration=4000, loss=[174459.94528618]\n",
      "||d|| = 2.8397301869174814\n",
      "Current iteration=5000, loss=[174700.39568214]\n",
      "||d|| = 1.0727366525916247\n",
      "Current iteration=6000, loss=[175387.19167337]\n",
      "||d|| = 0.8862156826399785\n",
      "Current iteration=7000, loss=[174305.50592982]\n",
      "||d|| = 0.7500416736686961\n",
      "Current iteration=8000, loss=[174975.57134729]\n",
      "||d|| = 0.9636122687013603\n",
      "Current iteration=9000, loss=[175494.54124079]\n",
      "||d|| = 4.164535961315001\n",
      "loss=[166861.22405087]\n",
      "lambda=0.0031622776601683794, Training loss=175163.701, Testing loss=35077.134\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[192178.54405627]\n",
      "||d|| = 1.7102584616375034\n",
      "Current iteration=1000, loss=[187070.1673447]\n",
      "||d|| = 4.277434461109334\n",
      "Current iteration=2000, loss=[185735.84652143]\n",
      "||d|| = 2.105960238469983\n",
      "Current iteration=3000, loss=[186333.77962923]\n",
      "||d|| = 0.8521400223131395\n",
      "Current iteration=4000, loss=[186364.86821188]\n",
      "||d|| = 0.7505026107960762\n",
      "Current iteration=5000, loss=[185443.0298151]\n",
      "||d|| = 0.9182593575537941\n",
      "Current iteration=6000, loss=[186163.0544687]\n",
      "||d|| = 1.3381738358274844\n",
      "Current iteration=7000, loss=[186315.58091869]\n",
      "||d|| = 1.1049132757844498\n",
      "Current iteration=8000, loss=[186766.04918078]\n",
      "||d|| = 4.232676835723373\n",
      "Current iteration=9000, loss=[185907.48724948]\n",
      "||d|| = 1.0883004101988492\n",
      "loss=[167373.2501753]\n",
      "lambda=0.0071968567300115215, Training loss=186262.556, Testing loss=37308.334\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[216281.50494753]\n",
      "||d|| = 0.8655381224604687\n",
      "Current iteration=1000, loss=[210987.7723704]\n",
      "||d|| = 1.7341521704736163\n",
      "Current iteration=2000, loss=[209422.03768237]\n",
      "||d|| = 4.012953759679057\n",
      "Current iteration=3000, loss=[209018.02187707]\n",
      "||d|| = 1.268328375018982\n",
      "Current iteration=4000, loss=[210236.91354088]\n",
      "||d|| = 0.9977386176987163\n",
      "Current iteration=5000, loss=[210130.80136157]\n",
      "||d|| = 1.3866830164181536\n",
      "Current iteration=6000, loss=[209694.84829753]\n",
      "||d|| = 2.3395539912021115\n",
      "Current iteration=7000, loss=[209997.40788817]\n",
      "||d|| = 1.5542206073372797\n",
      "Current iteration=8000, loss=[210279.81012211]\n",
      "||d|| = 4.112407303439851\n",
      "Current iteration=9000, loss=[209672.1515306]\n",
      "||d|| = 2.897254745658401\n",
      "loss=[167092.25010617]\n",
      "lambda=0.016378937069540647, Training loss=210118.435, Testing loss=42085.252\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[271136.13029826]\n",
      "||d|| = 0.9483496339446815\n",
      "Current iteration=1000, loss=[266030.14530878]\n",
      "||d|| = 1.6742374455436124\n",
      "Current iteration=2000, loss=[264931.15091336]\n",
      "||d|| = 1.3703575813144777\n",
      "Current iteration=3000, loss=[264152.08210145]\n",
      "||d|| = 1.3610131639601137\n",
      "Current iteration=4000, loss=[264594.73835943]\n",
      "||d|| = 0.7077120484856743\n",
      "Current iteration=5000, loss=[264639.89832428]\n",
      "||d|| = 4.129031033617864\n",
      "Current iteration=6000, loss=[264151.95373945]\n",
      "||d|| = 0.980751695536922\n",
      "Current iteration=7000, loss=[264092.82168845]\n",
      "||d|| = 1.019824362807536\n",
      "Current iteration=8000, loss=[263660.93478552]\n",
      "||d|| = 0.9567222845770662\n",
      "Current iteration=9000, loss=[264191.04948429]\n",
      "||d|| = 0.8061676815293978\n",
      "loss=[166740.6478794]\n",
      "lambda=0.037275937203149416, Training loss=264598.153, Testing loss=53011.813\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[395976.80592906]\n",
      "||d|| = 1.580624601198548\n",
      "Current iteration=1000, loss=[390918.33102625]\n",
      "||d|| = 0.8446051551269214\n",
      "Current iteration=2000, loss=[389550.75677266]\n",
      "||d|| = 0.9011377359181046\n",
      "Current iteration=3000, loss=[389782.8309837]\n",
      "||d|| = 1.573654846739687\n",
      "Current iteration=4000, loss=[389673.06972948]\n",
      "||d|| = 1.2950574839347255\n",
      "Current iteration=5000, loss=[389427.47473564]\n",
      "||d|| = 0.7197093690957935\n",
      "Current iteration=6000, loss=[389115.69017215]\n",
      "||d|| = 1.7896822431304622\n",
      "Current iteration=7000, loss=[389565.2735106]\n",
      "||d|| = 0.8795486398023088\n",
      "Current iteration=8000, loss=[389173.58500651]\n",
      "||d|| = 1.1157955965355775\n",
      "Current iteration=9000, loss=[389596.68913728]\n",
      "||d|| = 0.6860517625315583\n",
      "loss=[167655.4392395]\n",
      "lambda=0.08483428982440726, Training loss=390326.494, Testing loss=78232.707\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[680094.94897185]\n",
      "||d|| = 2.6627067285904022\n",
      "Current iteration=1000, loss=[675528.38496339]\n",
      "||d|| = 1.0699948082075703\n",
      "Current iteration=2000, loss=[674698.49230596]\n",
      "||d|| = 0.8406129809638427\n",
      "Current iteration=3000, loss=[674218.86313815]\n",
      "||d|| = 0.9512040026456446\n",
      "Current iteration=4000, loss=[673493.12037795]\n",
      "||d|| = 1.4183071013213284\n",
      "Current iteration=5000, loss=[673492.91759067]\n",
      "||d|| = 4.664564981799415\n",
      "Current iteration=6000, loss=[673805.36304486]\n",
      "||d|| = 0.9373217822052708\n",
      "Current iteration=7000, loss=[673172.77236397]\n",
      "||d|| = 1.201460330377661\n",
      "Current iteration=8000, loss=[674021.52316153]\n",
      "||d|| = 0.7571900101412813\n",
      "Current iteration=9000, loss=[673671.19188854]\n",
      "||d|| = 4.093729855166607\n",
      "loss=[166800.53766796]\n",
      "lambda=0.19306977288832497, Training loss=673605.551, Testing loss=135038.335\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[1326704.06733972]\n",
      "||d|| = 1.2243464976918232\n",
      "Current iteration=1000, loss=[1322132.46406816]\n",
      "||d|| = 0.7903695368716083\n",
      "Current iteration=2000, loss=[1321548.7142104]\n",
      "||d|| = 1.460139159748873\n",
      "Current iteration=3000, loss=[1320643.51224802]\n",
      "||d|| = 1.6942236135068813\n",
      "Current iteration=4000, loss=[1320541.03889993]\n",
      "||d|| = 3.370557896512458\n",
      "Current iteration=5000, loss=[1320556.11104971]\n",
      "||d|| = 1.1510650268129288\n",
      "Current iteration=6000, loss=[1320638.96012478]\n",
      "||d|| = 2.364199786482454\n",
      "Current iteration=7000, loss=[1320433.7322297]\n",
      "||d|| = 1.1698112980119728\n",
      "Current iteration=8000, loss=[1320952.78759399]\n",
      "||d|| = 4.847264751758059\n",
      "Current iteration=9000, loss=[1320788.93610306]\n",
      "||d|| = 1.4576776599327703\n",
      "loss=[167022.15985325]\n",
      "lambda=0.4393970560760795, Training loss=1320439.639, Testing loss=264747.998\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[2798286.79514004]\n",
      "||d|| = 0.8516641039245504\n",
      "Current iteration=1000, loss=[2794415.64664812]\n",
      "||d|| = 0.9344799994871188\n",
      "Current iteration=2000, loss=[2793077.83867476]\n",
      "||d|| = 1.466985252332965\n",
      "Current iteration=3000, loss=[2792881.49124866]\n",
      "||d|| = 0.9731223836877806\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-5, 0, 15)\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "ws = []\n",
    "tr_losses = []\n",
    "te_losses = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    w, tr_loss = reg_logistic_regression(y, tx, initial_w, lambda_, 10000, 0.001, method='newton')\n",
    "    ws.append(w)\n",
    "    te_loss = compute_loss(y_test, tx_test, w, lambda_=lambda_)\n",
    "    tr_losses.append(tr_loss[0])\n",
    "    te_losses.append(te_loss[0])\n",
    "\n",
    "    y_pred = predict_labels(w, tx_test)\n",
    "    training_accuracy = (y_pred == y_test).sum() / y_test.shape[0]\n",
    "\n",
    "    print(\"lambda={lambda_}, Training loss={tr:.3f}, Testing loss={te:.3f}\".format(\n",
    "        lambda_=lambda_, tr=tr_losses[ind], te=te_losses[ind]))\n",
    "    print(\"Training accuracy={acc:.3f}\".format(acc=training_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with summed jet columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = flatten_jet_features(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize x and add the 1s column\n",
    "x, mean_x, std_x = standardize(x)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "tx_poly = build_poly_matrix_vandermonde(tx, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "tx_train, y_train, tx_test, y_test = split_data(tx_poly, y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[271210.8712248]\n",
      "||d|| = 28.187006321784594\n",
      "Current iteration=1000, loss=[473276.92933884]\n",
      "||d|| = 0.0018141404419597484\n",
      "Current iteration=2000, loss=[471457.80712879]\n",
      "||d|| = 72.80105748739423\n",
      "Current iteration=3000, loss=[413814.03478361]\n",
      "||d|| = 2.20590705580624\n",
      "Current iteration=4000, loss=[382755.89768559]\n",
      "||d|| = 0.0011342766214547798\n",
      "Current iteration=5000, loss=[362612.0083851]\n",
      "||d|| = 6.656985821296442\n",
      "Current iteration=6000, loss=[347606.82277806]\n",
      "||d|| = 3.1595568788074604\n",
      "Current iteration=7000, loss=[325844.92175665]\n",
      "||d|| = 17.07173239625616\n",
      "Current iteration=8000, loss=[313553.25590884]\n",
      "||d|| = 3.19607849012299\n",
      "Current iteration=9000, loss=[295879.7846367]\n",
      "||d|| = 2.085721188752055\n",
      "loss=[153269.50450747]\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tx_poly.shape[1], 1))\n",
    "\n",
    "w, tr_loss = reg_logistic_regression(y_train, tx_train, initial_w, 0.001, 10000, 0.1, method='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68696"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tx_test)\n",
    "\n",
    "(y_pred == y_test).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Result with SGD, gamma = 0.1, degree 2, 10k iter: loss = 115658, test set accuracy = 0.68696\n",
    "- Result with Newton, gamma = 0.1, degree 2, 10k iter: loss = 126817, test set accuracy = 0.6554\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[139955.25046312]\n",
      "||d|| = 2.7131764979399953\n",
      "Current iteration=1000, loss=[129131.8432457]\n",
      "||d|| = 5.66480554398374\n",
      "Current iteration=2000, loss=[128452.4336647]\n",
      "||d|| = 3.358482380966959\n",
      "Current iteration=3000, loss=[128904.39914444]\n",
      "||d|| = 3.1543797487187564\n",
      "Current iteration=4000, loss=[128822.17365715]\n",
      "||d|| = 3.870159339983603\n",
      "Current iteration=5000, loss=[128707.81919923]\n",
      "||d|| = 2.157741942193498\n",
      "Current iteration=6000, loss=[128793.65809358]\n",
      "||d|| = 5.063492435556481\n",
      "Current iteration=7000, loss=[128838.25429689]\n",
      "||d|| = 2.116481931366886\n",
      "Current iteration=8000, loss=[128900.86620889]\n",
      "||d|| = 2.117993899172692\n",
      "Current iteration=9000, loss=[128713.22159776]\n",
      "||d|| = 2.10311382058259\n",
      "loss=[127524.0134424]\n",
      "lambda=1e-05, Training loss=128856.288, Testing loss=32318.584\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[141646.78530174]\n",
      "||d|| = 28.187607308561805\n",
      "Current iteration=1000, loss=[129927.61377292]\n",
      "||d|| = 2.4503415278703855\n",
      "Current iteration=2000, loss=[129955.37373749]\n",
      "||d|| = 16.759185115209764\n",
      "Current iteration=3000, loss=[129650.37548057]\n",
      "||d|| = 15.655089312718157\n",
      "Current iteration=4000, loss=[129785.45451969]\n",
      "||d|| = 15.901671251913625\n",
      "Current iteration=5000, loss=[129751.18795999]\n",
      "||d|| = 2.960402128659585\n",
      "Current iteration=6000, loss=[130031.33914559]\n",
      "||d|| = 3.1234916099546064\n",
      "Current iteration=7000, loss=[130065.11707873]\n",
      "||d|| = 3.182448257332641\n",
      "Current iteration=8000, loss=[130106.15652804]\n",
      "||d|| = 15.582948339156717\n",
      "Current iteration=9000, loss=[130300.83062946]\n",
      "||d|| = 71.34588967679093\n",
      "loss=[127446.04902751]\n",
      "lambda=2.2758459260747865e-05, Training loss=130464.672, Testing loss=32751.888\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[145496.45797302]\n",
      "||d|| = 2.531601252423577\n",
      "Current iteration=1000, loss=[135961.72006846]\n",
      "||d|| = 2.2491489849639956\n",
      "Current iteration=2000, loss=[135262.39969597]\n",
      "||d|| = 17.04478298630518\n",
      "Current iteration=3000, loss=[134629.85381974]\n",
      "||d|| = 17.000398862598132\n",
      "Current iteration=4000, loss=[134597.42473853]\n",
      "||d|| = 9.659519689431908\n",
      "Current iteration=5000, loss=[134142.27606445]\n",
      "||d|| = 2.098617240727721\n",
      "Current iteration=6000, loss=[133943.99611995]\n",
      "||d|| = 3.3207181441292417\n",
      "Current iteration=7000, loss=[133843.29143138]\n",
      "||d|| = 3.0122341740678484\n",
      "Current iteration=8000, loss=[134286.68063831]\n",
      "||d|| = 3.1047074104077357\n",
      "Current iteration=9000, loss=[134309.33305083]\n",
      "||d|| = 2.150694472922891\n",
      "loss=[127234.40136538]\n",
      "lambda=5.1794746792312125e-05, Training loss=134104.363, Testing loss=33727.445\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[154257.71983868]\n",
      "||d|| = 2.8235583435705425\n",
      "Current iteration=1000, loss=[143511.13278922]\n",
      "||d|| = 2.318591422602951\n",
      "Current iteration=2000, loss=[143374.5714952]\n",
      "||d|| = 7.2163015615268264\n",
      "Current iteration=3000, loss=[142974.05965127]\n",
      "||d|| = 16.072771188547804\n",
      "Current iteration=4000, loss=[142621.06839756]\n",
      "||d|| = 2.7262640481360054\n",
      "Current iteration=5000, loss=[142674.80110621]\n",
      "||d|| = 16.11527744693048\n",
      "Current iteration=6000, loss=[142682.23526049]\n",
      "||d|| = 4.513608085468858\n",
      "Current iteration=7000, loss=[142711.0062451]\n",
      "||d|| = 15.828198169218016\n",
      "Current iteration=8000, loss=[142866.89960979]\n",
      "||d|| = 15.658467900984968\n",
      "Current iteration=9000, loss=[142950.82089006]\n",
      "||d|| = 2.535128984523081\n",
      "loss=[127299.01931543]\n",
      "lambda=0.00011787686347935866, Training loss=142939.084, Testing loss=36092.490\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[174197.00196292]\n",
      "||d|| = 28.195522388880942\n",
      "Current iteration=1000, loss=[163987.47683102]\n",
      "||d|| = 3.360778352368673\n",
      "Current iteration=2000, loss=[164384.73956165]\n",
      "||d|| = 2.978331526102033\n",
      "Current iteration=3000, loss=[163990.32861992]\n",
      "||d|| = 2.138949343601188\n",
      "Current iteration=4000, loss=[163582.82269121]\n",
      "||d|| = 3.0064452207487364\n",
      "Current iteration=5000, loss=[163700.11932021]\n",
      "||d|| = 6.436336512607976\n",
      "Current iteration=6000, loss=[163387.91568769]\n",
      "||d|| = 7.895062916376463\n",
      "Current iteration=7000, loss=[163244.35687963]\n",
      "||d|| = 3.415178959493191\n",
      "Current iteration=8000, loss=[163090.36855506]\n",
      "||d|| = 3.345626184157206\n",
      "Current iteration=9000, loss=[162933.67582875]\n",
      "||d|| = 3.61536737075599\n",
      "loss=[127768.00899208]\n",
      "lambda=0.0002682695795279727, Training loss=163331.340, Testing loss=41553.186\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[219575.73595422]\n",
      "||d|| = 2.661379603033474\n",
      "Current iteration=1000, loss=[209392.57602096]\n",
      "||d|| = 2.1751328644119763\n",
      "Current iteration=2000, loss=[208941.29580196]\n",
      "||d|| = 12.896978726662867\n",
      "Current iteration=3000, loss=[208366.72434228]\n",
      "||d|| = 3.4325111172623988\n",
      "Current iteration=4000, loss=[208530.48464217]\n",
      "||d|| = 3.1754240630277164\n",
      "Current iteration=5000, loss=[208685.20780959]\n",
      "||d|| = 2.9736531321023976\n",
      "Current iteration=6000, loss=[208613.51220009]\n",
      "||d|| = 16.105198981874132\n",
      "Current iteration=7000, loss=[208121.57080953]\n",
      "||d|| = 2.2173009698257236\n",
      "Current iteration=8000, loss=[208137.99107842]\n",
      "||d|| = 2.3147126673968255\n",
      "Current iteration=9000, loss=[208311.24864369]\n",
      "||d|| = 2.138555191627609\n",
      "loss=[127320.49368544]\n",
      "lambda=0.0006105402296585327, Training loss=208265.847, Testing loss=53605.806\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[322850.74283876]\n",
      "||d|| = 2.640227287813802\n",
      "Current iteration=1000, loss=[312213.73256353]\n",
      "||d|| = 3.1994087763058583\n",
      "Current iteration=2000, loss=[311843.03321814]\n",
      "||d|| = 2.11019498985013\n",
      "Current iteration=3000, loss=[311373.77086799]\n",
      "||d|| = 40.67888602163192\n",
      "Current iteration=4000, loss=[310886.0808615]\n",
      "||d|| = 3.3405250444232086\n",
      "Current iteration=5000, loss=[310863.41473651]\n",
      "||d|| = 2.721713151913254\n",
      "Current iteration=6000, loss=[311003.80836217]\n",
      "||d|| = 4.908861620848498\n",
      "Current iteration=7000, loss=[310964.50532434]\n",
      "||d|| = 3.0417051456894106\n",
      "Current iteration=8000, loss=[311335.04403171]\n",
      "||d|| = 3.29749047933466\n",
      "Current iteration=9000, loss=[311360.80557244]\n",
      "||d|| = 2.477708565608854\n",
      "loss=[127165.66714912]\n",
      "lambda=0.0013894954943731374, Training loss=311386.749, Testing loss=81241.881\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[557888.74652228]\n",
      "||d|| = 3.141741270716292\n",
      "Current iteration=1000, loss=[546976.23340872]\n",
      "||d|| = 2.222886679277222\n",
      "Current iteration=2000, loss=[547099.20650191]\n",
      "||d|| = 2.134543622995221\n",
      "Current iteration=3000, loss=[547115.76110498]\n",
      "||d|| = 2.5889897538912034\n",
      "Current iteration=4000, loss=[547055.54452067]\n",
      "||d|| = 2.93331045843041\n",
      "Current iteration=5000, loss=[547015.11738666]\n",
      "||d|| = 2.1496146407469543\n",
      "Current iteration=6000, loss=[546857.96945031]\n",
      "||d|| = 16.17330238791353\n",
      "Current iteration=7000, loss=[546881.09412335]\n",
      "||d|| = 15.922266159710837\n",
      "Current iteration=8000, loss=[546972.45835623]\n",
      "||d|| = 2.9889978370068713\n",
      "Current iteration=9000, loss=[546948.4497043]\n",
      "||d|| = 17.124316897636604\n",
      "loss=[127608.49564979]\n",
      "lambda=0.0031622776601683794, Training loss=546864.646, Testing loss=144328.175\n",
      "Training accuracy=0.655\n",
      "Current iteration=0, loss=[1092799.02967818]\n",
      "||d|| = 2.538883905875992\n",
      "Current iteration=1000, loss=[1082824.02411356]\n",
      "||d|| = 2.219687631642439\n",
      "Current iteration=2000, loss=[1082877.86712605]\n",
      "||d|| = 6.401111125294508\n",
      "Current iteration=3000, loss=[1083026.88360993]\n",
      "||d|| = 3.7162915872277\n",
      "Current iteration=4000, loss=[1082425.46325812]\n",
      "||d|| = 3.214832161487799\n",
      "Current iteration=5000, loss=[1082027.21322238]\n",
      "||d|| = 8.18298265485076\n",
      "Current iteration=6000, loss=[1082126.15272303]\n",
      "||d|| = 3.085579893556429\n",
      "Current iteration=7000, loss=[1082224.85499888]\n",
      "||d|| = 3.0869006643457064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1ecf0672f1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training accuracy={acc:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mpoly_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-1ecf0672f1c6>\u001b[0m in \u001b[0;36mpoly_reg_logistic_regression\u001b[0;34m(tx, y, degree, ratio)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mte_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documenti/EPFL/M1/ML/projects/ml_project_1/src/logistic/not_req_impl.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, initial_w, lambda_, max_iters, gamma, method, ratio)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documenti/EPFL/M1/ML/projects/ml_project_1/src/logistic/not_req_impl.py\u001b[0m in \u001b[0;36mgradient_descent_step\u001b[0;34m(y, tx, w, gamma, lambda_, method)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Get loss, gradient, hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mrand_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documenti/EPFL/M1/ML/projects/ml_project_1/src/logistic/loss.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msumming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0my_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msumming\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_component\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def poly_reg_logistic_regression(tx, y, degree, ratio):\n",
    "    \n",
    "    tx_poly = build_poly_matrix_vandermonde(tx, degree)\n",
    "    tx_train, y_train, tx_test, y_test = split_data(tx_poly, y, ratio)\n",
    " \n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    initial_w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "    ws = []\n",
    "    tr_losses = []\n",
    "    te_losses = []\n",
    "\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "\n",
    "\n",
    "        w, tr_loss = reg_logistic_regression(y_train, tx_train, initial_w, lambda_, 10000, 0.1, method='newton')\n",
    "        ws.append(w)\n",
    "        te_loss = compute_loss(y_test, tx_test, w, lambda_=lambda_)\n",
    "        tr_losses.append(tr_loss.flatten()[0])\n",
    "        te_losses.append(te_loss.flatten()[0])\n",
    "\n",
    "        y_pred = predict_labels(w, tx_test)\n",
    "        training_accuracy = (y_pred == y_test).sum() / y_test.shape[0]\n",
    "\n",
    "        print(\"lambda={lambda_}, Training loss={tr:.3f}, Testing loss={te:.3f}\".format(\n",
    "            lambda_=lambda_, tr=tr_losses[ind], te=te_losses[ind]))\n",
    "        print(\"Training accuracy={acc:.3f}\".format(acc=training_accuracy))\n",
    "\n",
    "poly_reg_logistic_regression(tx, y, 2, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with different subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With quadratic polynomial expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('data/train.csv')\n",
    "init_col_n = x_raw.shape[1]\n",
    "init_col_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of columns indices\n",
    "curr_cols = list(range(x_raw.shape[1]))\n",
    "\n",
    "def rem_cols(lst, mask):\n",
    "    for c,x in enumerate(mask):\n",
    "        if not x:\n",
    "            lst.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "x, kept_columns = remove_incomplete_columns(x_raw)\n",
    "rem_cols(curr_cols, kept_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 138.47    51.655   97.827 ...  258.733    2.     113.497]\n",
      " [ 160.937   68.768  103.235 ...  164.546    1.      46.226]\n",
      " [-999.     162.172  125.953 ...  260.414    1.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ...  198.907    1.      41.992]\n",
      " [  94.951   19.362   68.812 ...  112.718    0.       0.   ]\n",
      " [-999.      72.756   70.831 ...   99.405    0.       0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, kept_columns_2 = remove_correlated_columns(x)\n",
    "rem_cols(curr_cols, kept_columns_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_columns_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 21, 22, 29]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.329\n",
      "0.0\n",
      "0.208\n",
      "46.104\n",
      "0.047\n",
      "-1.414\n",
      "-2.499\n",
      "-2.505\n",
      "-3.142\n"
     ]
    }
   ],
   "source": [
    "# log transform\n",
    "to_log_cols_idxs = [2, 3, 4, 6, 7, 8, 10, 13, 16]\n",
    "for i in to_log_cols_idxs:\n",
    "    print(x[:,i].min())\n",
    "    x[:,i] = np.apply_along_axis(lambda n: np.log(1 + abs(x[:,i].min()) + n), 0, x[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize x and add the 1s column\n",
    "x, mean_x, std_x = standardize(x)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly\n",
    "tx = build_poly_matrix_quadratic(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "tx_train, y_train, tx_test, y_test = split_data(tx, y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[173286.79513999]\n",
      "||d|| = 3.443337520133352\n",
      "Current iteration=1000, loss=[169331.57214641]\n",
      "||d|| = 8.587969507816975\n",
      "Current iteration=2000, loss=[140566.09295898]\n",
      "||d|| = 3.4237780837681195\n",
      "Current iteration=3000, loss=[139128.99164782]\n",
      "||d|| = 3.3300413210784403\n",
      "Current iteration=4000, loss=[142754.26234536]\n",
      "||d|| = 2.0571933740014594\n",
      "Current iteration=5000, loss=[133270.19730013]\n",
      "||d|| = 6.575845170786087\n",
      "Current iteration=6000, loss=[134135.57444219]\n",
      "||d|| = 5.557606082362086\n",
      "Current iteration=7000, loss=[132673.5156465]\n",
      "||d|| = 2.2204047658010806\n",
      "Current iteration=8000, loss=[133118.57044955]\n",
      "||d|| = 0.8498772901081524\n",
      "Current iteration=9000, loss=[137660.72928189]\n",
      "||d|| = 6.426500209839358\n",
      "loss=[132741.37886452]\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "w, tr_loss = reg_logistic_regression(y, tx, initial_w, 0, 10000, 0.05, method='sgd', ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result with gamma = 0.01 and 100k iter ||d|| = 0.44937032966833224 loss = [138897.157523], test set accuracy = 0.67562\n",
    "\n",
    "---\n",
    "\n",
    "Robbins-Monroe | Result with degree = 3, gamma = 0.0004 and 10k iter loss = [155654.57878558], test set accuracy = 0.65534\n",
    "\n",
    "Robbins-Monroe | Result with quadratic, gamma = 0.0004 and 10k iter loss = [155654.57878558], test set accuracy = 0.6554\n",
    "\n",
    "---\n",
    "\n",
    "Robbins-Monroe | quadratic, gamma = 0.01, ratio = 0.5, 10k iter | loss = [143340.81035078], test set accuracy = 0.6616\n",
    "\n",
    "---\n",
    "\n",
    "logistic w/ RM, log, quadratic | gamma = 0.01, ratio = 0.5, 10k iter | loss = [137580.1845174], test set accuracy = 0.69154\n",
    "\n",
    "logistic w/ RM, log, quadratic | gamma = 0.05, ratio = 0.5, 10k iter | loss = [132741.37886452], test set accuracy = 0.74846"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without mass column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('../data/train.csv')\n",
    "init_col_n = x_raw.shape[1]\n",
    "init_col_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jet_indexes = get_jet_indexes(x_raw)\n",
    "# x_jet_indexes = get_all(x_raw)\n",
    "x = x_raw\n",
    "# x_mass = np.zeros(x.shape[0])\n",
    "# x_mass[x[:, 0] == -999] = 1\n",
    "x[:, 0][x[:, 0] == -999] = np.median(x[:, 0][x[:, 0] != -999])\n",
    "# x = np.column_stack((x, x_mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 2.5787159411771086\n",
      "Current iteration=1000, loss=[48723.13405209]\n",
      "||d|| = 5.922103991217265\n",
      "Current iteration=2000, loss=[49316.23811431]\n",
      "||d|| = 7.187825262124538\n",
      "Current iteration=3000, loss=[47112.09800444]\n",
      "||d|| = 1.2774213848064553\n",
      "Current iteration=4000, loss=[47590.02996856]\n",
      "||d|| = 3.5928271124780147\n",
      "Current iteration=5000, loss=[46988.97556607]\n",
      "||d|| = 16.16833323616141\n",
      "Current iteration=6000, loss=[46356.64971734]\n",
      "||d|| = 0.9629467944887913\n",
      "Current iteration=7000, loss=[46352.58603949]\n",
      "||d|| = 5.306026661594372\n",
      "Current iteration=8000, loss=[46224.20597002]\n",
      "||d|| = 3.808306576732271\n",
      "Current iteration=9000, loss=[46568.19341752]\n",
      "||d|| = 4.211216331407385\n",
      "loss=[46164.79383636]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 3.469929013599797\n",
      "Current iteration=1000, loss=[46925.37148927]\n",
      "||d|| = 7.224795563582372\n",
      "Current iteration=2000, loss=[47209.80848357]\n",
      "||d|| = 4.342158945304195\n",
      "Current iteration=3000, loss=[46341.56359176]\n",
      "||d|| = 15.456095701346426\n",
      "Current iteration=4000, loss=[47651.34008829]\n",
      "||d|| = 5.372523873349237\n",
      "Current iteration=5000, loss=[46258.85766943]\n",
      "||d|| = 1.8032377558289623\n",
      "Current iteration=6000, loss=[46427.70961522]\n",
      "||d|| = 24.159523778103228\n",
      "Current iteration=7000, loss=[46394.54055078]\n",
      "||d|| = 1.2924403319261182\n",
      "Current iteration=8000, loss=[46866.79935991]\n",
      "||d|| = 6.981705565186253\n",
      "Current iteration=9000, loss=[46658.227454]\n",
      "||d|| = 14.163770990615276\n",
      "loss=[46173.11263277]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 27.992528293731898\n",
      "Current iteration=1000, loss=[48906.91629198]\n",
      "||d|| = 21.508198768770644\n",
      "Current iteration=2000, loss=[47886.0536523]\n",
      "||d|| = 2.0969636136288194\n",
      "Current iteration=3000, loss=[47016.23159504]\n",
      "||d|| = 13.694161412386837\n",
      "Current iteration=4000, loss=[47515.38623812]\n",
      "||d|| = 4.339064511117669\n",
      "Current iteration=5000, loss=[46904.84406765]\n",
      "||d|| = 4.42646358539247\n",
      "Current iteration=6000, loss=[46602.05420809]\n",
      "||d|| = 56.51322883505971\n",
      "Current iteration=7000, loss=[46888.59897048]\n",
      "||d|| = 5.158088301112536\n",
      "Current iteration=8000, loss=[46526.50133113]\n",
      "||d|| = 6.964727961836589\n",
      "Current iteration=9000, loss=[47135.17038274]\n",
      "||d|| = 10.628987327974805\n",
      "loss=[46394.53142186]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 2.036458522693078\n",
      "Current iteration=1000, loss=[47870.82200113]\n",
      "||d|| = 4.841822759331324\n",
      "Current iteration=2000, loss=[47368.92938939]\n",
      "||d|| = 42.25187190432178\n",
      "Current iteration=3000, loss=[48200.84535455]\n",
      "||d|| = 1.7266054042214585\n",
      "Current iteration=4000, loss=[46730.81074322]\n",
      "||d|| = 31.40155439008291\n",
      "Current iteration=5000, loss=[46796.14171304]\n",
      "||d|| = 3.4706567525171828\n",
      "Current iteration=6000, loss=[49816.91840589]\n",
      "||d|| = 3.93509232299413\n",
      "Current iteration=7000, loss=[46532.54472856]\n",
      "||d|| = 5.017509220040997\n",
      "Current iteration=8000, loss=[48425.84653437]\n",
      "||d|| = 1.3038533334300266\n",
      "Current iteration=9000, loss=[46262.51744359]\n",
      "||d|| = 3.1853625095899556\n",
      "loss=[47064.24895484]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 11.889904202333884\n",
      "Current iteration=1000, loss=[48494.33210178]\n",
      "||d|| = 26.585195200153237\n",
      "Current iteration=2000, loss=[46717.27675359]\n",
      "||d|| = 13.327760713795882\n",
      "Current iteration=3000, loss=[46499.72747727]\n",
      "||d|| = 6.7959017221985265\n",
      "Current iteration=4000, loss=[46962.59182494]\n",
      "||d|| = 1.2445696688349954\n",
      "Current iteration=5000, loss=[46366.92760999]\n",
      "||d|| = 5.48021085642297\n",
      "Current iteration=6000, loss=[46266.35449867]\n",
      "||d|| = 6.3134680675340356\n",
      "Current iteration=7000, loss=[46431.71608114]\n",
      "||d|| = 2.145349813828544\n",
      "Current iteration=8000, loss=[46549.46221941]\n",
      "||d|| = 2.7551930528369266\n",
      "Current iteration=9000, loss=[46265.3159627]\n",
      "||d|| = 4.748758852742881\n",
      "loss=[47233.8440488]\n",
      "lambda=0.000000, Training accuracy=0.745, Testing accuracy=0.744\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 3.121963526314799\n",
      "Current iteration=1000, loss=[41165.05611255]\n",
      "||d|| = 13.254878315873516\n",
      "Current iteration=2000, loss=[43507.64823672]\n",
      "||d|| = 3.1515936211012026\n",
      "Current iteration=3000, loss=[40655.42665713]\n",
      "||d|| = 7.631929194835587\n",
      "Current iteration=4000, loss=[40945.67543527]\n",
      "||d|| = 4.327054348076598\n",
      "Current iteration=5000, loss=[42780.44927355]\n",
      "||d|| = 3.9339101602798885\n",
      "Current iteration=6000, loss=[41335.651971]\n",
      "||d|| = 3.584501461726176\n",
      "Current iteration=7000, loss=[40348.22538344]\n",
      "||d|| = 6.405053635315871\n",
      "Current iteration=8000, loss=[43869.77956288]\n",
      "||d|| = 4.664883538577875\n",
      "Current iteration=9000, loss=[40418.19113137]\n",
      "||d|| = 43.613286629113304\n",
      "loss=[40376.16042165]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 4.377252468290054\n",
      "Current iteration=1000, loss=[45205.40845738]\n",
      "||d|| = 2.5265585155285324\n",
      "Current iteration=2000, loss=[40583.09412127]\n",
      "||d|| = 14.489193198447266\n",
      "Current iteration=3000, loss=[40446.20761806]\n",
      "||d|| = 9.456402931038864\n",
      "Current iteration=4000, loss=[40732.63680769]\n",
      "||d|| = 2.682408512857374\n",
      "Current iteration=5000, loss=[40798.15920238]\n",
      "||d|| = 32.902394396631564\n",
      "Current iteration=6000, loss=[40698.66675545]\n",
      "||d|| = 4.9660374349291025\n",
      "Current iteration=7000, loss=[41457.11794676]\n",
      "||d|| = 5.371812988215759\n",
      "Current iteration=8000, loss=[40476.79942003]\n",
      "||d|| = 5.444090151558866\n",
      "Current iteration=9000, loss=[40311.08313223]\n",
      "||d|| = 4.239243476688029\n",
      "loss=[40327.40610148]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 21.05297406410774\n",
      "Current iteration=1000, loss=[41157.31297263]\n",
      "||d|| = 9.747298094812285\n",
      "Current iteration=2000, loss=[40774.3206772]\n",
      "||d|| = 7.86914041235554\n",
      "Current iteration=3000, loss=[42950.81192293]\n",
      "||d|| = 7.591253643906173\n",
      "Current iteration=4000, loss=[40678.78379189]\n",
      "||d|| = 2.2628169703876058\n",
      "Current iteration=5000, loss=[40942.69120528]\n",
      "||d|| = 3.313862537074612\n",
      "Current iteration=6000, loss=[40439.90950289]\n",
      "||d|| = 11.410834918563392\n",
      "Current iteration=7000, loss=[41388.718373]\n",
      "||d|| = 14.547081502031268\n",
      "Current iteration=8000, loss=[40463.4175937]\n",
      "||d|| = 4.966653167641798\n",
      "Current iteration=9000, loss=[40627.53357158]\n",
      "||d|| = 1.2012980179367427\n",
      "loss=[40347.03349475]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 14.422935680267145\n",
      "Current iteration=1000, loss=[40665.22489013]\n",
      "||d|| = 7.1123154972318\n",
      "Current iteration=2000, loss=[42513.70807759]\n",
      "||d|| = 10.245173799622528\n",
      "Current iteration=3000, loss=[41297.01214919]\n",
      "||d|| = 10.396445090772966\n",
      "Current iteration=4000, loss=[40368.75828455]\n",
      "||d|| = 2.407990044430058\n",
      "Current iteration=5000, loss=[40410.53731114]\n",
      "||d|| = 5.389088447689139\n",
      "Current iteration=6000, loss=[47267.06963987]\n",
      "||d|| = 4.034911317518237\n",
      "Current iteration=7000, loss=[41125.24606365]\n",
      "||d|| = 8.225225478162622\n",
      "Current iteration=8000, loss=[41235.08230324]\n",
      "||d|| = 3.341000643572774\n",
      "Current iteration=9000, loss=[40643.73064182]\n",
      "||d|| = 6.969487291920795\n",
      "loss=[40520.07883864]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 10.319166066009084\n",
      "Current iteration=1000, loss=[40946.59989402]\n",
      "||d|| = 3.6128644911816465\n",
      "Current iteration=2000, loss=[42810.756847]\n",
      "||d|| = 1.9522876427565075\n",
      "Current iteration=3000, loss=[40740.38417503]\n",
      "||d|| = 9.848233909711615\n",
      "Current iteration=4000, loss=[40406.01745399]\n",
      "||d|| = 34.67932305575409\n",
      "Current iteration=5000, loss=[40406.14364007]\n",
      "||d|| = 3.3787341750623208\n",
      "Current iteration=6000, loss=[42205.19522988]\n",
      "||d|| = 8.329767221882921\n",
      "Current iteration=7000, loss=[40320.73930144]\n",
      "||d|| = 6.407715915572022\n",
      "Current iteration=8000, loss=[40314.5914315]\n",
      "||d|| = 18.307027787804767\n",
      "Current iteration=9000, loss=[40531.91938829]\n",
      "||d|| = 8.09941853203313\n",
      "loss=[40441.57268376]\n",
      "lambda=0.000000, Training accuracy=0.642, Testing accuracy=0.645\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 12.089654858986156\n",
      "Current iteration=1000, loss=[45175.10646347]\n",
      "||d|| = 5.054438368054257\n",
      "Current iteration=2000, loss=[38635.776109]\n",
      "||d|| = 10.045383621535569\n",
      "Current iteration=3000, loss=[38439.35425602]\n",
      "||d|| = 12.12326608032745\n",
      "Current iteration=4000, loss=[38474.17089925]\n",
      "||d|| = 21.177882345285877\n",
      "Current iteration=5000, loss=[38769.6123826]\n",
      "||d|| = 5.265502834471641\n",
      "Current iteration=6000, loss=[38362.34701878]\n",
      "||d|| = 4.737471262202617\n",
      "Current iteration=7000, loss=[38375.5650865]\n",
      "||d|| = 6.648976912851143\n",
      "Current iteration=8000, loss=[38658.99899247]\n",
      "||d|| = 5.335963842827721\n",
      "Current iteration=9000, loss=[40336.21839731]\n",
      "||d|| = 6.746837616523258\n",
      "loss=[38363.3215185]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 7.829934147767218\n",
      "Current iteration=1000, loss=[39930.24279587]\n",
      "||d|| = 4.662717333624727\n",
      "Current iteration=2000, loss=[41014.44263467]\n",
      "||d|| = 6.956143968240886\n",
      "Current iteration=3000, loss=[38365.7855172]\n",
      "||d|| = 14.561558726525059\n",
      "Current iteration=4000, loss=[38337.01666243]\n",
      "||d|| = 17.81858576225167\n",
      "Current iteration=5000, loss=[38473.54859798]\n",
      "||d|| = 11.90665635410887\n",
      "Current iteration=6000, loss=[38248.30401199]\n",
      "||d|| = 7.6730087985536395\n",
      "Current iteration=7000, loss=[39590.53152205]\n",
      "||d|| = 14.246085143672865\n",
      "Current iteration=8000, loss=[38582.80803811]\n",
      "||d|| = 14.427261721887477\n",
      "Current iteration=9000, loss=[38849.88457083]\n",
      "||d|| = 11.884740841274956\n",
      "loss=[38353.62487554]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 4.505718668962651\n",
      "Current iteration=1000, loss=[39208.14294721]\n",
      "||d|| = 5.429189591278388\n",
      "Current iteration=2000, loss=[45260.19214242]\n",
      "||d|| = 10.421629699936135\n",
      "Current iteration=3000, loss=[38843.89468552]\n",
      "||d|| = 6.2481442273568835\n",
      "Current iteration=4000, loss=[39073.85464572]\n",
      "||d|| = 4.518079357655175\n",
      "Current iteration=5000, loss=[38443.3362025]\n",
      "||d|| = 5.02183581927184\n",
      "Current iteration=6000, loss=[38185.45025067]\n",
      "||d|| = 5.741448403133525\n",
      "Current iteration=7000, loss=[38926.05361073]\n",
      "||d|| = 21.112672787818045\n",
      "Current iteration=8000, loss=[40545.04320361]\n",
      "||d|| = 3.45674945631444\n",
      "Current iteration=9000, loss=[38899.05687345]\n",
      "||d|| = 4.877813477263171\n",
      "loss=[38169.54584987]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 26.961745092608062\n",
      "Current iteration=1000, loss=[38784.39810299]\n",
      "||d|| = 60.25223452171123\n",
      "Current iteration=2000, loss=[40973.55928362]\n",
      "||d|| = 9.788614895481873\n",
      "Current iteration=3000, loss=[39475.34700916]\n",
      "||d|| = 6.987938786886605\n",
      "Current iteration=4000, loss=[41447.75968068]\n",
      "||d|| = 5.777349381986947\n",
      "Current iteration=5000, loss=[38172.60038659]\n",
      "||d|| = 7.791404910074382\n",
      "Current iteration=6000, loss=[38239.6401395]\n",
      "||d|| = 5.390761606564263\n",
      "Current iteration=7000, loss=[38285.13018806]\n",
      "||d|| = 9.989184347474655\n",
      "Current iteration=8000, loss=[43716.53249506]\n",
      "||d|| = 2.305996177051096\n",
      "Current iteration=9000, loss=[38197.10599047]\n",
      "||d|| = 12.320651194990935\n",
      "loss=[38257.01616809]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 15.10551543420244\n",
      "Current iteration=1000, loss=[39298.97431713]\n",
      "||d|| = 28.037465698371616\n",
      "Current iteration=2000, loss=[39049.51417229]\n",
      "||d|| = 5.538630886987204\n",
      "Current iteration=3000, loss=[38808.06731305]\n",
      "||d|| = 10.009947076655594\n",
      "Current iteration=4000, loss=[38404.28777121]\n",
      "||d|| = 6.337185410144358\n",
      "Current iteration=5000, loss=[38421.59469711]\n",
      "||d|| = 5.314272030858169\n",
      "Current iteration=6000, loss=[38211.90500647]\n",
      "||d|| = 8.716718706409322\n",
      "Current iteration=7000, loss=[39158.29607108]\n",
      "||d|| = 6.033133542404114\n",
      "Current iteration=8000, loss=[38202.29395665]\n",
      "||d|| = 2.924860473281003\n",
      "Current iteration=9000, loss=[38231.05919317]\n",
      "||d|| = 6.772717334554552\n",
      "loss=[38356.65493099]\n",
      "lambda=0.000000, Training accuracy=0.577, Testing accuracy=0.573\n"
     ]
    }
   ],
   "source": [
    "ws, te_accs, tr_accs, te_losses, tr_losses = [], [], [], [], []\n",
    "lambda_ = 0\n",
    "k = 5\n",
    "\n",
    "for i in x_jet_indexes:\n",
    "    \n",
    "    y_i = y[x_jet_indexes[i]]\n",
    "    tx_i = x[x_jet_indexes[i]]\n",
    "    tx_del = np.delete(tx_i, jet_indexes[i], axis=1)\n",
    "    \n",
    "    for li in log_indexes:\n",
    "       # print(tx_del[:,li].min())\n",
    "       tx_del[:,li] = np.apply_along_axis(lambda n: np.log(1 + abs(tx_del[:,li].min()) + n), 0, tx_del[:,li])\n",
    "    \n",
    "    tx_std = standardize(tx_del)[0]\n",
    "    tx_poly = build_poly_matrix_quadratic(tx_std)\n",
    "    tx = np.c_[np.ones((y_i.shape[0], 1)), tx_poly]\n",
    "    \n",
    "    initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    k_indices = build_k_indices(y_i, k, 1)\n",
    "    \n",
    "    te_accs_k, tr_accs_k, te_losses_k, tr_losses_k, ws_k = [], [], [], [], []\n",
    "    \n",
    "    for k_ in range(k):\n",
    "        \n",
    "        test_indices = k_indices[k_]\n",
    "        train_indices = np.setdiff1d(k_indices.flatten(), test_indices)\n",
    "\n",
    "        y_train = y_i[train_indices]\n",
    "        x_train = tx[train_indices]\n",
    "        y_test = y_i[test_indices]\n",
    "        x_test = tx[test_indices]\n",
    "\n",
    "        # Ridge linear\n",
    "        w, loss_tr_k = reg_logistic_regression(y_train, x_train, initial_w, lambda_, 10000, 0.01, method='sgd', ratio=0.5)\n",
    "\n",
    "        # Calculate the loss for test data\n",
    "        loss_te_k = compute_loss(y_test, x_test, w)\n",
    "        \n",
    "        acc_tr_k = compute_accuracy(x_train, w, y_train, mode='logistic')\n",
    "        acc_te_k = compute_accuracy(x_test, w, y_test, mode='logistic')\n",
    "        \n",
    "        te_accs_k.append(acc_te_k)\n",
    "        tr_accs_k.append(acc_tr_k)\n",
    "        te_losses_k.append(np.math.sqrt(2 * loss_te_k))\n",
    "        tr_losses_k.append(np.math.sqrt(2 * loss_tr_k))\n",
    "\n",
    "        ws_k.append(w)\n",
    "        \n",
    "\n",
    "    te_accs.append(np.mean(te_accs_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_accs.append(np.mean(acc_te_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    te_losses.append(np.mean(te_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_losses.append(np.mean(tr_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    ws.append(np.mean(ws_k, axis=0))\n",
    "\n",
    "    print(\"lambda={l:.6f}, Training accuracy={tr:.3f}, Testing accuracy={te:.3f}\".format(\n",
    "           l=lambda_, tr=tr_accs[i] / y_i.shape[0], te=te_accs[i] / y_i.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With mass column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('../data/train.csv')\n",
    "init_col_n = x_raw.shape[1]\n",
    "init_col_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jet_indexes = get_jet_indexes(x_raw)\n",
    "# x_jet_indexes = get_all(x_raw)\n",
    "x = x_raw\n",
    "x_mass = np.zeros(x.shape[0])\n",
    "x_mass[x[:, 0] == -999] = 1\n",
    "x[:, 0][x[:, 0] == -999] = np.median(x[:, 0][x[:, 0] != -999])\n",
    "x = np.column_stack((x, x_mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 3.4241362425983177\n",
      "Current iteration=1000, loss=[49820.68382129]\n",
      "||d|| = 12.675298374652264\n",
      "Current iteration=2000, loss=[47261.71399705]\n",
      "||d|| = 2.795636048590469\n",
      "Current iteration=3000, loss=[46613.63864134]\n",
      "||d|| = 7.766423477049506\n",
      "Current iteration=4000, loss=[47082.00467998]\n",
      "||d|| = 11.5013856680073\n",
      "Current iteration=5000, loss=[46583.53475557]\n",
      "||d|| = 14.287333971330819\n",
      "Current iteration=6000, loss=[46299.55575193]\n",
      "||d|| = 3.871322223252055\n",
      "Current iteration=7000, loss=[46312.30588604]\n",
      "||d|| = 1.5363500856181416\n",
      "Current iteration=8000, loss=[46412.66924231]\n",
      "||d|| = 1.2021993539628\n",
      "Current iteration=9000, loss=[46184.71128872]\n",
      "||d|| = 8.766659316064718\n",
      "loss=[46976.23589851]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 10.246944292417446\n",
      "Current iteration=1000, loss=[48318.82925819]\n",
      "||d|| = 8.505885622731759\n",
      "Current iteration=2000, loss=[47809.82206217]\n",
      "||d|| = 5.960986018032392\n",
      "Current iteration=3000, loss=[46996.7634807]\n",
      "||d|| = 2.990073873218402\n",
      "Current iteration=4000, loss=[47083.48757497]\n",
      "||d|| = 11.179307457803494\n",
      "Current iteration=5000, loss=[46449.52492867]\n",
      "||d|| = 3.3918332530518085\n",
      "Current iteration=6000, loss=[46559.35974143]\n",
      "||d|| = 3.5751730003551043\n",
      "Current iteration=7000, loss=[46286.60844266]\n",
      "||d|| = 21.433297141884076\n",
      "Current iteration=8000, loss=[47278.00722121]\n",
      "||d|| = 4.8534121338208545\n",
      "Current iteration=9000, loss=[46353.83360372]\n",
      "||d|| = 12.906202734459686\n",
      "loss=[46151.68320496]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 3.508892865271336\n",
      "Current iteration=1000, loss=[49688.30497392]\n",
      "||d|| = 3.833248853343473\n",
      "Current iteration=2000, loss=[47420.52584499]\n",
      "||d|| = 10.50344608178783\n",
      "Current iteration=3000, loss=[47589.50957841]\n",
      "||d|| = 34.166062657442936\n",
      "Current iteration=4000, loss=[47354.82710778]\n",
      "||d|| = 8.715188445033508\n",
      "Current iteration=5000, loss=[47180.51632774]\n",
      "||d|| = 2.614639034398196\n",
      "Current iteration=6000, loss=[47037.0717513]\n",
      "||d|| = 4.12373791293291\n",
      "Current iteration=7000, loss=[47027.51711958]\n",
      "||d|| = 18.83944128390408\n",
      "Current iteration=8000, loss=[46469.53002355]\n",
      "||d|| = 1.8489122599287755\n",
      "Current iteration=9000, loss=[46461.55003146]\n",
      "||d|| = 6.3118510268578465\n",
      "loss=[47136.09687407]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 11.856395946972075\n",
      "Current iteration=1000, loss=[51268.56601582]\n",
      "||d|| = 15.419395616744895\n",
      "Current iteration=2000, loss=[47189.33729919]\n",
      "||d|| = 6.027715063448476\n",
      "Current iteration=3000, loss=[47091.22875393]\n",
      "||d|| = 8.681207416434743\n",
      "Current iteration=4000, loss=[46530.36588459]\n",
      "||d|| = 15.32395649705091\n",
      "Current iteration=5000, loss=[51367.08004608]\n",
      "||d|| = 1.9721857766600466\n",
      "Current iteration=6000, loss=[47822.14828045]\n",
      "||d|| = 4.377294334197134\n",
      "Current iteration=7000, loss=[46872.82747879]\n",
      "||d|| = 2.443873618663275\n",
      "Current iteration=8000, loss=[46357.7955906]\n",
      "||d|| = 2.883776917020428\n",
      "Current iteration=9000, loss=[46228.7878853]\n",
      "||d|| = 11.720419580021911\n",
      "loss=[46345.44468943]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 9.687859511735061\n",
      "Current iteration=1000, loss=[47790.46450241]\n",
      "||d|| = 5.04091541245699\n",
      "Current iteration=2000, loss=[46814.32255547]\n",
      "||d|| = 8.787725404470136\n",
      "Current iteration=3000, loss=[46759.8352866]\n",
      "||d|| = 3.5434792824255634\n",
      "Current iteration=4000, loss=[47410.88960821]\n",
      "||d|| = 9.324238376246823\n",
      "Current iteration=5000, loss=[48163.95880096]\n",
      "||d|| = 43.16481822034942\n",
      "Current iteration=6000, loss=[46861.34969092]\n",
      "||d|| = 81.87542038674901\n",
      "Current iteration=7000, loss=[46569.66626498]\n",
      "||d|| = 7.939746420775442\n",
      "Current iteration=8000, loss=[46355.2271993]\n",
      "||d|| = 1.5179006730112727\n",
      "Current iteration=9000, loss=[46277.98379521]\n",
      "||d|| = 1.5137232341831794\n",
      "loss=[46334.58519708]\n",
      "lambda=0.000000, Training accuracy=0.744, Testing accuracy=0.744\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 8.55587749328718\n",
      "Current iteration=1000, loss=[40737.45321881]\n",
      "||d|| = 4.76772247031763\n",
      "Current iteration=2000, loss=[41997.43025669]\n",
      "||d|| = 3.8561032310487513\n",
      "Current iteration=3000, loss=[45989.38767877]\n",
      "||d|| = 6.871095545621004\n",
      "Current iteration=4000, loss=[41141.14600627]\n",
      "||d|| = 2.8920578649255253\n",
      "Current iteration=5000, loss=[41988.73649895]\n",
      "||d|| = 8.852006230536928\n",
      "Current iteration=6000, loss=[40385.86593184]\n",
      "||d|| = 2.490595076731974\n",
      "Current iteration=7000, loss=[41117.03558001]\n",
      "||d|| = 3.7242400408061322\n",
      "Current iteration=8000, loss=[41112.98259175]\n",
      "||d|| = 39.32622474684313\n",
      "Current iteration=9000, loss=[40667.75747458]\n",
      "||d|| = 3.7363797635711777\n",
      "loss=[40706.25606118]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 14.014932766377\n",
      "Current iteration=1000, loss=[41265.1188368]\n",
      "||d|| = 4.511546819227367\n",
      "Current iteration=2000, loss=[40888.66578311]\n",
      "||d|| = 6.929676587220022\n",
      "Current iteration=3000, loss=[40652.49708639]\n",
      "||d|| = 87.04300858791001\n",
      "Current iteration=4000, loss=[40765.25572008]\n",
      "||d|| = 28.78034333464464\n",
      "Current iteration=5000, loss=[40416.45165588]\n",
      "||d|| = 4.841843535301959\n",
      "Current iteration=6000, loss=[40439.90357669]\n",
      "||d|| = 4.3766166104617295\n",
      "Current iteration=7000, loss=[40807.8185408]\n",
      "||d|| = 9.246255194999364\n",
      "Current iteration=8000, loss=[42354.59366317]\n",
      "||d|| = 12.610797175220876\n",
      "Current iteration=9000, loss=[42659.59973432]\n",
      "||d|| = 4.0860843603835395\n",
      "loss=[40418.55931378]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 19.2187904392528\n",
      "Current iteration=1000, loss=[42508.80034794]\n",
      "||d|| = 4.831480556793572\n",
      "Current iteration=2000, loss=[43150.56737098]\n",
      "||d|| = 7.086311199248344\n",
      "Current iteration=3000, loss=[42080.61534896]\n",
      "||d|| = 9.344597629745198\n",
      "Current iteration=4000, loss=[40822.41289216]\n",
      "||d|| = 6.6432705254076\n",
      "Current iteration=5000, loss=[40585.01282369]\n",
      "||d|| = 32.79962335841364\n",
      "Current iteration=6000, loss=[41025.12348221]\n",
      "||d|| = 17.255726058974044\n",
      "Current iteration=7000, loss=[40440.13544058]\n",
      "||d|| = 12.207703769882166\n",
      "Current iteration=8000, loss=[40440.52202895]\n",
      "||d|| = 15.165524348267159\n",
      "Current iteration=9000, loss=[40537.99782451]\n",
      "||d|| = 15.99015182099926\n",
      "loss=[40384.02544833]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 3.696811737446659\n",
      "Current iteration=1000, loss=[42341.94505905]\n",
      "||d|| = 6.104933770013996\n",
      "Current iteration=2000, loss=[40458.58657215]\n",
      "||d|| = 3.751546480028294\n",
      "Current iteration=3000, loss=[42092.35530518]\n",
      "||d|| = 16.041352865237066\n",
      "Current iteration=4000, loss=[40664.87285754]\n",
      "||d|| = 8.091187470408084\n",
      "Current iteration=5000, loss=[41657.7573499]\n",
      "||d|| = 7.116085668101949\n",
      "Current iteration=6000, loss=[40355.25045894]\n",
      "||d|| = 8.31200744042113\n",
      "Current iteration=7000, loss=[40822.78277883]\n",
      "||d|| = 9.466121570381437\n",
      "Current iteration=8000, loss=[40386.30398706]\n",
      "||d|| = 5.082186611933307\n",
      "Current iteration=9000, loss=[41252.24728882]\n",
      "||d|| = 22.75991338402421\n",
      "loss=[40678.31846226]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 18.66898830466372\n",
      "Current iteration=1000, loss=[43573.43963704]\n",
      "||d|| = 13.43240940822775\n",
      "Current iteration=2000, loss=[43628.06245118]\n",
      "||d|| = 5.655377518404222\n",
      "Current iteration=3000, loss=[40480.30968814]\n",
      "||d|| = 8.767630133991036\n",
      "Current iteration=4000, loss=[40596.18682463]\n",
      "||d|| = 17.794780348627054\n",
      "Current iteration=5000, loss=[40478.03614706]\n",
      "||d|| = 8.580274941748176\n",
      "Current iteration=6000, loss=[40394.78935983]\n",
      "||d|| = 5.488662609312045\n",
      "Current iteration=7000, loss=[40545.65411073]\n",
      "||d|| = 7.421529262268812\n",
      "Current iteration=8000, loss=[40365.53054343]\n",
      "||d|| = 10.607162408275638\n",
      "Current iteration=9000, loss=[40530.22844944]\n",
      "||d|| = 6.521267295943077\n",
      "loss=[40341.76289409]\n",
      "lambda=0.000000, Training accuracy=0.642, Testing accuracy=0.646\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 20.753324856307533\n",
      "Current iteration=1000, loss=[40732.22566575]\n",
      "||d|| = 4.822888039568025\n",
      "Current iteration=2000, loss=[38869.03026407]\n",
      "||d|| = 5.180294661547168\n",
      "Current iteration=3000, loss=[38452.1704135]\n",
      "||d|| = 6.394207750256021\n",
      "Current iteration=4000, loss=[38461.15305767]\n",
      "||d|| = 4.937702494151039\n",
      "Current iteration=5000, loss=[39071.86064975]\n",
      "||d|| = 9.283028973843495\n",
      "Current iteration=6000, loss=[38752.61217496]\n",
      "||d|| = 5.953791131994445\n",
      "Current iteration=7000, loss=[38145.77391149]\n",
      "||d|| = 4.901778694666989\n",
      "Current iteration=8000, loss=[38488.80261794]\n",
      "||d|| = 3.477102325752085\n",
      "Current iteration=9000, loss=[38226.3171755]\n",
      "||d|| = 11.087577094840848\n",
      "loss=[38096.53054743]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 8.593999246350004\n",
      "Current iteration=1000, loss=[39671.77005474]\n",
      "||d|| = 4.601244223576725\n",
      "Current iteration=2000, loss=[39757.93702159]\n",
      "||d|| = 8.103302265747699\n",
      "Current iteration=3000, loss=[40283.32740778]\n",
      "||d|| = 3.1023959830644974\n",
      "Current iteration=4000, loss=[39479.65744118]\n",
      "||d|| = 7.8692857951085005\n",
      "Current iteration=5000, loss=[40152.82533255]\n",
      "||d|| = 5.174959205560793\n",
      "Current iteration=6000, loss=[39627.43306998]\n",
      "||d|| = 6.090307041066672\n",
      "Current iteration=7000, loss=[38440.01051941]\n",
      "||d|| = 4.983061371572049\n",
      "Current iteration=8000, loss=[38425.50459398]\n",
      "||d|| = 3.9315377531850455\n",
      "Current iteration=9000, loss=[38170.87310144]\n",
      "||d|| = 4.4525844494658084\n",
      "loss=[39802.05000216]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 18.222278929368922\n",
      "Current iteration=1000, loss=[41274.06628266]\n",
      "||d|| = 5.707889481538137\n",
      "Current iteration=2000, loss=[38467.93853361]\n",
      "||d|| = 5.238524616459153\n",
      "Current iteration=3000, loss=[38618.34186868]\n",
      "||d|| = 3.1926059971625858\n",
      "Current iteration=4000, loss=[38921.25469337]\n",
      "||d|| = 11.368420817966184\n",
      "Current iteration=5000, loss=[38168.49516434]\n",
      "||d|| = 4.495395799037589\n",
      "Current iteration=6000, loss=[38061.56671597]\n",
      "||d|| = 6.031002920039724\n",
      "Current iteration=7000, loss=[39545.66989138]\n",
      "||d|| = 7.825307052193355\n",
      "Current iteration=8000, loss=[39178.62738493]\n",
      "||d|| = 6.844919329488618\n",
      "Current iteration=9000, loss=[38146.22786046]\n",
      "||d|| = 4.194026274693493\n",
      "loss=[38525.6046931]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 4.7801284396252415\n",
      "Current iteration=1000, loss=[42648.9101584]\n",
      "||d|| = 5.534989394701833\n",
      "Current iteration=2000, loss=[42030.00109147]\n",
      "||d|| = 71.11775547476758\n",
      "Current iteration=3000, loss=[38394.21998672]\n",
      "||d|| = 5.812785260454684\n",
      "Current iteration=4000, loss=[38676.23007234]\n",
      "||d|| = 24.962479592938763\n",
      "Current iteration=5000, loss=[38201.16230415]\n",
      "||d|| = 11.72331759007349\n",
      "Current iteration=6000, loss=[39643.7555477]\n",
      "||d|| = 3.080397699758339\n",
      "Current iteration=7000, loss=[39049.95242726]\n",
      "||d|| = 3.209811129849938\n",
      "Current iteration=8000, loss=[40387.15953105]\n",
      "||d|| = 4.8262665973390675\n",
      "Current iteration=9000, loss=[37986.75080471]\n",
      "||d|| = 5.020672812040634\n",
      "loss=[42356.69056615]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 26.690701989622546\n",
      "Current iteration=1000, loss=[39972.75303692]\n",
      "||d|| = 7.841577846346793\n",
      "Current iteration=2000, loss=[38859.74938555]\n",
      "||d|| = 33.61800438440989\n",
      "Current iteration=3000, loss=[38807.12156851]\n",
      "||d|| = 9.321353920140428\n",
      "Current iteration=4000, loss=[39465.61181829]\n",
      "||d|| = 5.975968257042185\n",
      "Current iteration=5000, loss=[39876.09085754]\n",
      "||d|| = 101.64032580494279\n",
      "Current iteration=6000, loss=[38514.78597252]\n",
      "||d|| = 8.7687354780285\n",
      "Current iteration=7000, loss=[39098.50111142]\n",
      "||d|| = 22.881427573156493\n",
      "Current iteration=8000, loss=[38268.21318325]\n",
      "||d|| = 3.709336668825347\n",
      "Current iteration=9000, loss=[38267.72295875]\n",
      "||d|| = 8.587048788651677\n",
      "loss=[38830.80789095]\n",
      "lambda=0.000000, Training accuracy=0.557, Testing accuracy=0.575\n"
     ]
    }
   ],
   "source": [
    "ws, te_accs, tr_accs, te_losses, tr_losses = [], [], [], [], []\n",
    "lambda_ = 0\n",
    "k = 5\n",
    "\n",
    "for i in x_jet_indexes:\n",
    "    \n",
    "    y_i = y[x_jet_indexes[i]]\n",
    "    tx_i = x[x_jet_indexes[i]]\n",
    "    tx_del = np.delete(tx_i, jet_indexes[i], axis=1)\n",
    "    \n",
    "    for li in log_indexes:\n",
    "       # print(tx_del[:,li].min())\n",
    "       tx_del[:,li] = np.apply_along_axis(lambda n: np.log(1 + abs(tx_del[:,li].min()) + n), 0, tx_del[:,li])\n",
    "    \n",
    "    tx_std = standardize(tx_del)[0]\n",
    "    tx_poly = build_poly_matrix_quadratic(tx_std)\n",
    "    tx = np.c_[np.ones((y_i.shape[0], 1)), tx_poly]\n",
    "    \n",
    "    initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    k_indices = build_k_indices(y_i, k, 1)\n",
    "    \n",
    "    te_accs_k, tr_accs_k, te_losses_k, tr_losses_k, ws_k = [], [], [], [], []\n",
    "    \n",
    "    for k_ in range(k):\n",
    "        \n",
    "        test_indices = k_indices[k_]\n",
    "        train_indices = np.setdiff1d(k_indices.flatten(), test_indices)\n",
    "\n",
    "        y_train = y_i[train_indices]\n",
    "        x_train = tx[train_indices]\n",
    "        y_test = y_i[test_indices]\n",
    "        x_test = tx[test_indices]\n",
    "\n",
    "        # Ridge linear\n",
    "        w, loss_tr_k = reg_logistic_regression(y_train, x_train, initial_w, lambda_, 10000, 0.01, method='sgd', ratio=0.5)\n",
    "\n",
    "        # Calculate the loss for test data\n",
    "        loss_te_k = compute_loss(y_test, x_test, w)\n",
    "        \n",
    "        acc_tr_k = compute_accuracy(x_train, w, y_train, mode='logistic')\n",
    "        acc_te_k = compute_accuracy(x_test, w, y_test, mode='logistic')\n",
    "        \n",
    "        te_accs_k.append(acc_te_k)\n",
    "        tr_accs_k.append(acc_tr_k)\n",
    "        te_losses_k.append(np.math.sqrt(2 * loss_te_k))\n",
    "        tr_losses_k.append(np.math.sqrt(2 * loss_tr_k))\n",
    "\n",
    "        ws_k.append(w)\n",
    "        \n",
    "\n",
    "    te_accs.append(np.mean(te_accs_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_accs.append(np.mean(acc_te_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    te_losses.append(np.mean(te_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_losses.append(np.mean(tr_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    ws.append(np.mean(ws_k, axis=0))\n",
    "\n",
    "    print(\"lambda={l:.6f}, Training accuracy={tr:.3f}, Testing accuracy={te:.3f}\".format(\n",
    "           l=lambda_, tr=tr_accs[i] / y_i.shape[0], te=te_accs[i] / y_i.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('../data/train.csv')\n",
    "init_col_n = x_raw.shape[1]\n",
    "init_col_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jet_indexes = get_jet_indexes(x_raw)\n",
    "# x_jet_indexes = get_all(x_raw)\n",
    "x = x_raw\n",
    "x_mass = np.zeros(x.shape[0])\n",
    "x_mass[x[:, 0] == -999] = 1\n",
    "x[:, 0][x[:, 0] == -999] = np.median(x[:, 0][x[:, 0] != -999])\n",
    "x = np.column_stack((x, x_mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 6.476739119738675\n",
      "Current iteration=1000, loss=[38069.98310069]\n",
      "||d|| = 10.417890193416616\n",
      "Current iteration=2000, loss=[38339.38567851]\n",
      "||d|| = 4.652161093775247\n",
      "Current iteration=3000, loss=[37393.65378116]\n",
      "||d|| = 2.4425804190750786\n",
      "Current iteration=4000, loss=[37099.2471176]\n",
      "||d|| = 1.818833262249398\n",
      "Current iteration=5000, loss=[36853.63971436]\n",
      "||d|| = 2.1610816054541933\n",
      "Current iteration=6000, loss=[36460.39148303]\n",
      "||d|| = 1.0174059679392067\n",
      "Current iteration=7000, loss=[36424.98055793]\n",
      "||d|| = 6.904450684956423\n",
      "Current iteration=8000, loss=[36196.6889689]\n",
      "||d|| = 2.1274999535267787\n",
      "Current iteration=9000, loss=[36449.78074782]\n",
      "||d|| = 1.6229486924072816\n",
      "loss=[36227.84596917]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 4.726917060209904\n",
      "Current iteration=1000, loss=[38463.39625739]\n",
      "||d|| = 1.961411138204492\n",
      "Current iteration=2000, loss=[38102.62244801]\n",
      "||d|| = 5.856510404057953\n",
      "Current iteration=3000, loss=[38376.45051467]\n",
      "||d|| = 1.4193332011935473\n",
      "Current iteration=4000, loss=[37297.55095472]\n",
      "||d|| = 2.2066084361739198\n",
      "Current iteration=5000, loss=[37465.86805897]\n",
      "||d|| = 4.385903207942973\n",
      "Current iteration=6000, loss=[36916.17474677]\n",
      "||d|| = 2.4486283727105835\n",
      "Current iteration=7000, loss=[36441.27143264]\n",
      "||d|| = 2.0384058462823487\n",
      "Current iteration=8000, loss=[36233.492884]\n",
      "||d|| = 8.785143894224435\n",
      "Current iteration=9000, loss=[36487.97853187]\n",
      "||d|| = 5.946310807778464\n",
      "loss=[36003.47749408]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 4.7979915432586795\n",
      "Current iteration=1000, loss=[38461.34624398]\n",
      "||d|| = 7.166285233021003\n",
      "Current iteration=2000, loss=[37782.79118509]\n",
      "||d|| = 9.747097018292806\n",
      "Current iteration=3000, loss=[37286.37362362]\n",
      "||d|| = 2.6580581977355626\n",
      "Current iteration=4000, loss=[37065.3049748]\n",
      "||d|| = 2.895783170562011\n",
      "Current iteration=5000, loss=[37779.22409144]\n",
      "||d|| = 0.38898909967596634\n",
      "Current iteration=6000, loss=[36764.32426434]\n",
      "||d|| = 1.5540574351072582\n",
      "Current iteration=7000, loss=[36854.30241457]\n",
      "||d|| = 2.0300135838550846\n",
      "Current iteration=8000, loss=[36666.87563504]\n",
      "||d|| = 1.115697143847165\n",
      "Current iteration=9000, loss=[36745.50738868]\n",
      "||d|| = 1.2598935633164785\n",
      "loss=[36410.67722582]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 4.471302881672084\n",
      "Current iteration=1000, loss=[39397.03711107]\n",
      "||d|| = 1.9113176492152768\n",
      "Current iteration=2000, loss=[38112.18054169]\n",
      "||d|| = 3.464767214929188\n",
      "Current iteration=3000, loss=[37692.91866252]\n",
      "||d|| = 18.33938633512527\n",
      "Current iteration=4000, loss=[37589.99496369]\n",
      "||d|| = 2.5810226053496392\n",
      "Current iteration=5000, loss=[36912.18515607]\n",
      "||d|| = 11.041233242695883\n",
      "Current iteration=6000, loss=[36740.84839798]\n",
      "||d|| = 6.9199294528195265\n",
      "Current iteration=7000, loss=[36764.97819922]\n",
      "||d|| = 0.7897478196998088\n",
      "Current iteration=8000, loss=[36465.31050061]\n",
      "||d|| = 2.8171798383231788\n",
      "Current iteration=9000, loss=[36479.97834485]\n",
      "||d|| = 1.3377188704415954\n",
      "loss=[36227.82656962]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 4.142688233130077\n",
      "Current iteration=1000, loss=[38694.19068947]\n",
      "||d|| = 16.03104866658064\n",
      "Current iteration=2000, loss=[37245.51871422]\n",
      "||d|| = 2.0540848759133477\n",
      "Current iteration=3000, loss=[37073.96783486]\n",
      "||d|| = 0.9554110853837753\n",
      "Current iteration=4000, loss=[36792.90599362]\n",
      "||d|| = 0.9703717445963906\n",
      "Current iteration=5000, loss=[36933.35871773]\n",
      "||d|| = 2.0528668929631957\n",
      "Current iteration=6000, loss=[36489.01935535]\n",
      "||d|| = 1.6658003144722788\n",
      "Current iteration=7000, loss=[36413.39697724]\n",
      "||d|| = 1.8102917784280952\n",
      "Current iteration=8000, loss=[36342.93804961]\n",
      "||d|| = 0.9818652880317035\n",
      "Current iteration=9000, loss=[36159.42547305]\n",
      "||d|| = 3.165295882176401\n",
      "loss=[36448.86538234]\n",
      "lambda=0.000000, Training accuracy=0.757, Testing accuracy=0.776\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 7.542419084720914\n",
      "Current iteration=1000, loss=[48395.82370354]\n",
      "||d|| = 6.000219591615584\n",
      "Current iteration=2000, loss=[36823.09766797]\n",
      "||d|| = 2.3096804289474715\n",
      "Current iteration=3000, loss=[36359.1186252]\n",
      "||d|| = 16.45340901492483\n",
      "Current iteration=4000, loss=[36311.59872849]\n",
      "||d|| = 6.930436275691372\n",
      "Current iteration=5000, loss=[37694.50878957]\n",
      "||d|| = 18.994683186030343\n",
      "Current iteration=6000, loss=[35716.64820744]\n",
      "||d|| = 9.996151470730563\n",
      "Current iteration=7000, loss=[36026.02340903]\n",
      "||d|| = 3.0429815480903386\n",
      "Current iteration=8000, loss=[36182.26214766]\n",
      "||d|| = 5.8020332051370005\n",
      "Current iteration=9000, loss=[35294.03991269]\n",
      "||d|| = 6.381098513723713\n",
      "loss=[35289.4711136]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 6.19032209682852\n",
      "Current iteration=1000, loss=[37139.6933562]\n",
      "||d|| = 6.3590523799488174\n",
      "Current iteration=2000, loss=[36681.19490109]\n",
      "||d|| = 2.689671391206824\n",
      "Current iteration=3000, loss=[36623.84257325]\n",
      "||d|| = 1.9574626211886637\n",
      "Current iteration=4000, loss=[36711.67786014]\n",
      "||d|| = 6.50385852416217\n",
      "Current iteration=5000, loss=[36382.64535948]\n",
      "||d|| = 6.49758882114681\n",
      "Current iteration=6000, loss=[35511.7774761]\n",
      "||d|| = 5.024631537959729\n",
      "Current iteration=7000, loss=[35559.28444882]\n",
      "||d|| = 6.6878048491503925\n",
      "Current iteration=8000, loss=[35465.01387248]\n",
      "||d|| = 4.7581304341064685\n",
      "Current iteration=9000, loss=[35655.03482186]\n",
      "||d|| = 2.781233461067332\n",
      "loss=[35369.03590772]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 38.73185725774576\n",
      "Current iteration=1000, loss=[37739.88291147]\n",
      "||d|| = 26.028961334876815\n",
      "Current iteration=2000, loss=[36935.40738378]\n",
      "||d|| = 5.255273634689861\n",
      "Current iteration=3000, loss=[36230.19989472]\n",
      "||d|| = 5.799674634966796\n",
      "Current iteration=4000, loss=[36252.63978571]\n",
      "||d|| = 12.080786089999439\n",
      "Current iteration=5000, loss=[35921.28022176]\n",
      "||d|| = 1.929783188480257\n",
      "Current iteration=6000, loss=[35883.95395004]\n",
      "||d|| = 3.4875147555790673\n",
      "Current iteration=7000, loss=[35547.93040408]\n",
      "||d|| = 10.056180360011476\n",
      "Current iteration=8000, loss=[35646.15355226]\n",
      "||d|| = 5.838072621207293\n",
      "Current iteration=9000, loss=[37930.29551819]\n",
      "||d|| = 2.3794510336307257\n",
      "loss=[35344.38795798]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 5.422234482836186\n",
      "Current iteration=1000, loss=[40089.30623478]\n",
      "||d|| = 8.462638223729975\n",
      "Current iteration=2000, loss=[36896.30832801]\n",
      "||d|| = 5.237829463839263\n",
      "Current iteration=3000, loss=[36486.24058352]\n",
      "||d|| = 4.488092439897901\n",
      "Current iteration=4000, loss=[36958.36659891]\n",
      "||d|| = 6.362099942646932\n",
      "Current iteration=5000, loss=[35955.82328649]\n",
      "||d|| = 6.874969544565821\n",
      "Current iteration=6000, loss=[36279.31524986]\n",
      "||d|| = 2.9414496187843646\n",
      "Current iteration=7000, loss=[36674.36300738]\n",
      "||d|| = 5.3568987865102535\n",
      "Current iteration=8000, loss=[35767.37066192]\n",
      "||d|| = 2.9729746321128436\n",
      "Current iteration=9000, loss=[35647.06436026]\n",
      "||d|| = 4.497003740253281\n",
      "loss=[35971.32744474]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 11.239862794812705\n",
      "Current iteration=1000, loss=[39911.85636629]\n",
      "||d|| = 66.178156884955\n",
      "Current iteration=2000, loss=[38090.22247284]\n",
      "||d|| = 12.711550797488337\n",
      "Current iteration=3000, loss=[36475.36325455]\n",
      "||d|| = 3.4828059761453427\n",
      "Current iteration=4000, loss=[36185.42474103]\n",
      "||d|| = 4.646799038781379\n",
      "Current iteration=5000, loss=[35919.29299513]\n",
      "||d|| = 2.6621372756612165\n",
      "Current iteration=6000, loss=[36080.30939034]\n",
      "||d|| = 1.9502270501589958\n",
      "Current iteration=7000, loss=[35651.16623735]\n",
      "||d|| = 7.8991772489454\n",
      "Current iteration=8000, loss=[35825.77617715]\n",
      "||d|| = 5.1402854758283905\n",
      "Current iteration=9000, loss=[35448.52037637]\n",
      "||d|| = 3.4633246093925445\n",
      "loss=[35269.57081364]\n",
      "lambda=0.000000, Training accuracy=0.670, Testing accuracy=0.666\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 5.342251175081957\n",
      "Current iteration=1000, loss=[37019.17219633]\n",
      "||d|| = 12.218694883213837\n",
      "Current iteration=2000, loss=[35394.91935582]\n",
      "||d|| = 6.945787392410734\n",
      "Current iteration=3000, loss=[34548.09125549]\n",
      "||d|| = 2.81455952157248\n",
      "Current iteration=4000, loss=[34581.2295347]\n",
      "||d|| = 2.7068638633806112\n",
      "Current iteration=5000, loss=[34971.5425527]\n",
      "||d|| = 2.9804290961952513\n",
      "Current iteration=6000, loss=[34591.71628734]\n",
      "||d|| = 3.1229079490089275\n",
      "Current iteration=7000, loss=[33802.50709917]\n",
      "||d|| = 3.876046485905579\n",
      "Current iteration=8000, loss=[33788.15194672]\n",
      "||d|| = 0.004885829432739559\n",
      "Current iteration=9000, loss=[33771.93576779]\n",
      "||d|| = 3.9465663290246367\n",
      "loss=[33536.05806025]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 17.019987040282423\n",
      "Current iteration=1000, loss=[36187.80323556]\n",
      "||d|| = 16.950600359444916\n",
      "Current iteration=2000, loss=[35804.08059322]\n",
      "||d|| = 4.130235724482431\n",
      "Current iteration=3000, loss=[34357.1086727]\n",
      "||d|| = 2.709349665929329\n",
      "Current iteration=4000, loss=[34554.99332111]\n",
      "||d|| = 8.535608667044498\n",
      "Current iteration=5000, loss=[34521.25248469]\n",
      "||d|| = 8.748176564937248\n",
      "Current iteration=6000, loss=[33791.54444131]\n",
      "||d|| = 7.670509869415616\n",
      "Current iteration=7000, loss=[33905.38169833]\n",
      "||d|| = 0.001375109254406772\n",
      "Current iteration=8000, loss=[34481.54735822]\n",
      "||d|| = 5.646064227304624\n",
      "Current iteration=9000, loss=[33524.80190742]\n",
      "||d|| = 2.165281772743361\n",
      "loss=[33427.63743238]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 3.421983523474694\n",
      "Current iteration=1000, loss=[34923.01223044]\n",
      "||d|| = 2.051444315299996\n",
      "Current iteration=2000, loss=[34549.35594626]\n",
      "||d|| = 3.067049795758812\n",
      "Current iteration=3000, loss=[34159.14722016]\n",
      "||d|| = 7.852977581040591\n",
      "Current iteration=4000, loss=[34189.63899587]\n",
      "||d|| = 6.524895745000641\n",
      "Current iteration=5000, loss=[33787.74664422]\n",
      "||d|| = 21.277091012570374\n",
      "Current iteration=6000, loss=[34286.83819021]\n",
      "||d|| = 2.3117279023628496\n",
      "Current iteration=7000, loss=[33788.74643289]\n",
      "||d|| = 2.2134410084656597\n",
      "Current iteration=8000, loss=[33479.83118747]\n",
      "||d|| = 1.9636552073973792\n",
      "Current iteration=9000, loss=[33907.54781579]\n",
      "||d|| = 2.071524735983647\n",
      "loss=[33435.69394088]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 164.414487447205\n",
      "Current iteration=1000, loss=[50832.14834455]\n",
      "||d|| = 3.536745844912266\n",
      "Current iteration=2000, loss=[44271.48208603]\n",
      "||d|| = 2.151921051979165\n",
      "Current iteration=3000, loss=[41666.8402813]\n",
      "||d|| = 5.547555813676561\n",
      "Current iteration=4000, loss=[39775.31399254]\n",
      "||d|| = 14.615516461633431\n",
      "Current iteration=5000, loss=[38574.54980078]\n",
      "||d|| = 2.765758525809251\n",
      "Current iteration=6000, loss=[36868.26417745]\n",
      "||d|| = 3.5371405077831426\n",
      "Current iteration=7000, loss=[36180.43483077]\n",
      "||d|| = 7.925764625393072\n",
      "Current iteration=8000, loss=[35743.525561]\n",
      "||d|| = 2.431218054860121e-13\n",
      "Current iteration=9000, loss=[35571.42103708]\n",
      "||d|| = 2.631313391335239\n",
      "loss=[35044.04228502]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 28.576955619331887\n",
      "Current iteration=1000, loss=[36749.32340486]\n",
      "||d|| = 4.098097424293736\n",
      "Current iteration=2000, loss=[35718.63629712]\n",
      "||d|| = 3.1987562147872435\n",
      "Current iteration=3000, loss=[34686.50047443]\n",
      "||d|| = 2.3350178221568934\n",
      "Current iteration=4000, loss=[34587.33342146]\n",
      "||d|| = 3.3479951395781833\n",
      "Current iteration=5000, loss=[34381.66478973]\n",
      "||d|| = 8.625410808159424\n",
      "Current iteration=6000, loss=[34102.14369965]\n",
      "||d|| = 3.035207923278929\n",
      "Current iteration=7000, loss=[34027.91243551]\n",
      "||d|| = 8.510813880233627\n",
      "Current iteration=8000, loss=[34451.63803074]\n",
      "||d|| = 1.816184107272984\n",
      "Current iteration=9000, loss=[34290.738912]\n",
      "||d|| = 25.56759095440978\n",
      "loss=[33663.08412985]\n",
      "lambda=0.000000, Training accuracy=0.667, Testing accuracy=0.664\n"
     ]
    }
   ],
   "source": [
    "ws, te_accs, tr_accs, te_losses, tr_losses = [], [], [], [], []\n",
    "lambda_ = 0\n",
    "k = 5\n",
    "\n",
    "for i in x_jet_indexes:\n",
    "    \n",
    "    y_i = y[x_jet_indexes[i]]\n",
    "    tx_i = x[x_jet_indexes[i]]\n",
    "    tx_del = np.delete(tx_i, jet_indexes[i], axis=1)\n",
    "    \n",
    "    # for li in log_indexes:\n",
    "    #   # print(tx_del[:,li].min())\n",
    "    #   tx_del[:,li] = np.apply_along_axis(lambda n: np.log(1 + abs(tx_del[:,li].min()) + n), 0, tx_del[:,li])\n",
    "    \n",
    "    tx_std = standardize(tx_del)[0]\n",
    "    tx_poly = build_poly_matrix_quadratic(tx_std)\n",
    "    tx = np.c_[np.ones((y_i.shape[0], 1)), tx_poly]\n",
    "    \n",
    "    initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    k_indices = build_k_indices(y_i, k, 1)\n",
    "    \n",
    "    te_accs_k, tr_accs_k, te_losses_k, tr_losses_k, ws_k = [], [], [], [], []\n",
    "    \n",
    "    for k_ in range(k):\n",
    "        \n",
    "        test_indices = k_indices[k_]\n",
    "        train_indices = np.setdiff1d(k_indices.flatten(), test_indices)\n",
    "\n",
    "        y_train = y_i[train_indices]\n",
    "        x_train = tx[train_indices]\n",
    "        y_test = y_i[test_indices]\n",
    "        x_test = tx[test_indices]\n",
    "\n",
    "        # Ridge linear\n",
    "        w, loss_tr_k = reg_logistic_regression(y_train, x_train, initial_w, lambda_, 10000, 0.01, method='sgd', ratio=0.5)\n",
    "\n",
    "        # Calculate the loss for test data\n",
    "        loss_te_k = compute_loss(y_test, x_test, w)\n",
    "        \n",
    "        acc_tr_k = compute_accuracy(x_train, w, y_train, mode='logistic')\n",
    "        acc_te_k = compute_accuracy(x_test, w, y_test, mode='logistic')\n",
    "        \n",
    "        te_accs_k.append(acc_te_k)\n",
    "        tr_accs_k.append(acc_tr_k)\n",
    "        te_losses_k.append(np.math.sqrt(2 * loss_te_k))\n",
    "        tr_losses_k.append(np.math.sqrt(2 * loss_tr_k))\n",
    "\n",
    "        ws_k.append(w)\n",
    "        \n",
    "\n",
    "    te_accs.append(np.mean(te_accs_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_accs.append(np.mean(acc_te_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    te_losses.append(np.mean(te_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_losses.append(np.mean(tr_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    ws.append(np.mean(ws_k, axis=0))\n",
    "\n",
    "    print(\"lambda={l:.6f}, Training accuracy={tr:.3f}, Testing accuracy={te:.3f}\".format(\n",
    "           l=lambda_, tr=tr_accs[i] / y_i.shape[0], te=te_accs[i] / y_i.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without logarithm and mass column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('../data/train.csv')\n",
    "init_col_n = x_raw.shape[1]\n",
    "init_col_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jet_indexes = get_jet_indexes(x_raw)\n",
    "# x_jet_indexes = get_all(x_raw)\n",
    "x = x_raw\n",
    "# x_mass = np.zeros(x.shape[0])\n",
    "# x_mass[x[:, 0] == -999] = 1\n",
    "x[:, 0][x[:, 0] == -999] = np.median(x[:, 0][x[:, 0] != -999])\n",
    "# x = np.column_stack((x, x_mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 8.625644532854063\n",
      "Current iteration=1000, loss=[38411.42511902]\n",
      "||d|| = 8.294951655184045\n",
      "Current iteration=2000, loss=[37840.09690223]\n",
      "||d|| = 3.4985747752798755\n",
      "Current iteration=3000, loss=[37398.20672033]\n",
      "||d|| = 1.8017233146020377\n",
      "Current iteration=4000, loss=[37204.65339111]\n",
      "||d|| = 10.249003794965903\n",
      "Current iteration=5000, loss=[37765.08317311]\n",
      "||d|| = 1.9573549262185745\n",
      "Current iteration=6000, loss=[36765.68436233]\n",
      "||d|| = 6.052507492681298\n",
      "Current iteration=7000, loss=[36769.59367373]\n",
      "||d|| = 4.6978093253947995\n",
      "Current iteration=8000, loss=[36557.40860552]\n",
      "||d|| = 1.2476174996270983\n",
      "Current iteration=9000, loss=[36562.32876424]\n",
      "||d|| = 5.455720183403461\n",
      "loss=[36276.21585838]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 19.41304749530535\n",
      "Current iteration=1000, loss=[38749.29699402]\n",
      "||d|| = 2.0336516351783835\n",
      "Current iteration=2000, loss=[38340.02406117]\n",
      "||d|| = 4.891001359336784\n",
      "Current iteration=3000, loss=[37485.8866449]\n",
      "||d|| = 1.0368221506426667\n",
      "Current iteration=4000, loss=[37302.53318798]\n",
      "||d|| = 10.291803407933214\n",
      "Current iteration=5000, loss=[37614.99268208]\n",
      "||d|| = 6.077048381026317\n",
      "Current iteration=6000, loss=[36739.12734679]\n",
      "||d|| = 2.4355021304892683\n",
      "Current iteration=7000, loss=[36483.31149967]\n",
      "||d|| = 7.146976170098881\n",
      "Current iteration=8000, loss=[36335.0522525]\n",
      "||d|| = 2.9685707384057913\n",
      "Current iteration=9000, loss=[36373.03039415]\n",
      "||d|| = 3.221528474492072\n",
      "loss=[36140.32003619]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 8.60412544860066\n",
      "Current iteration=1000, loss=[124308.39211014]\n",
      "||d|| = 7.8202354937037395\n",
      "Current iteration=2000, loss=[47738.77536657]\n",
      "||d|| = 3.114031778496306\n",
      "Current iteration=3000, loss=[44776.31898389]\n",
      "||d|| = 3.7329329082967173\n",
      "Current iteration=4000, loss=[42092.95580132]\n",
      "||d|| = 1.1128175129733529\n",
      "Current iteration=5000, loss=[41676.35362252]\n",
      "||d|| = 23.095585059095683\n",
      "Current iteration=6000, loss=[40529.7208239]\n",
      "||d|| = 1.8982006174407537\n",
      "Current iteration=7000, loss=[39407.32826032]\n",
      "||d|| = 7.7870489802374125\n",
      "Current iteration=8000, loss=[39096.39958539]\n",
      "||d|| = 1.4591559144488289\n",
      "Current iteration=9000, loss=[38187.69628665]\n",
      "||d|| = 10.877871008723165\n",
      "loss=[37923.73221121]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 8.278898404566883\n",
      "Current iteration=1000, loss=[39447.81558862]\n",
      "||d|| = 3.2140692017448886\n",
      "Current iteration=2000, loss=[38370.00207113]\n",
      "||d|| = 6.555868288572574\n",
      "Current iteration=3000, loss=[38168.24462321]\n",
      "||d|| = 6.683189075855741\n",
      "Current iteration=4000, loss=[37796.15595619]\n",
      "||d|| = 2.409536895819867\n",
      "Current iteration=5000, loss=[37380.11634]\n",
      "||d|| = 2.322713471524038\n",
      "Current iteration=6000, loss=[37151.09095877]\n",
      "||d|| = 1.1516269581387644\n",
      "Current iteration=7000, loss=[37023.20275104]\n",
      "||d|| = 7.058210633855636\n",
      "Current iteration=8000, loss=[37479.97644822]\n",
      "||d|| = 2.7982572573163784\n",
      "Current iteration=9000, loss=[37176.30036626]\n",
      "||d|| = 2.711760869674533\n",
      "loss=[40165.18569074]\n",
      "Current iteration=0, loss=[55401.8678478]\n",
      "||d|| = 5.208935589491456\n",
      "Current iteration=1000, loss=[39451.9927334]\n",
      "||d|| = 2.6545356626128394\n",
      "Current iteration=2000, loss=[37806.34714715]\n",
      "||d|| = 1.2603471958868837\n",
      "Current iteration=3000, loss=[37268.84473189]\n",
      "||d|| = 11.615734427045966\n",
      "Current iteration=4000, loss=[37096.71992575]\n",
      "||d|| = 2.7096824081775517\n",
      "Current iteration=5000, loss=[36740.95176985]\n",
      "||d|| = 1.6769535144933825\n",
      "Current iteration=6000, loss=[36666.90207599]\n",
      "||d|| = 1.6346835256781973\n",
      "Current iteration=7000, loss=[36681.28188657]\n",
      "||d|| = 6.388060417653753\n",
      "Current iteration=8000, loss=[36668.62918884]\n",
      "||d|| = 1.8037678329209677\n",
      "Current iteration=9000, loss=[36325.97819142]\n",
      "||d|| = 1.8121647190619024\n",
      "loss=[37036.6790279]\n",
      "lambda=0.000000, Training accuracy=0.756, Testing accuracy=0.769\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 10.300157441644402\n",
      "Current iteration=1000, loss=[37215.77987]\n",
      "||d|| = 4.289941570128123\n",
      "Current iteration=2000, loss=[36401.03585697]\n",
      "||d|| = 2.517304909074249\n",
      "Current iteration=3000, loss=[36092.01596417]\n",
      "||d|| = 3.101193391252472\n",
      "Current iteration=4000, loss=[36830.9140226]\n",
      "||d|| = 7.715440454482474\n",
      "Current iteration=5000, loss=[37021.58695211]\n",
      "||d|| = 11.407624025205083\n",
      "Current iteration=6000, loss=[35541.55013507]\n",
      "||d|| = 5.444259928505196\n",
      "Current iteration=7000, loss=[36402.73379784]\n",
      "||d|| = 6.052474507825968\n",
      "Current iteration=8000, loss=[35882.71996896]\n",
      "||d|| = 6.349797826495333\n",
      "Current iteration=9000, loss=[35678.56863232]\n",
      "||d|| = 3.457273360617183\n",
      "loss=[35213.76070422]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 5.473214263829696\n",
      "Current iteration=1000, loss=[38167.32441228]\n",
      "||d|| = 10.991649921192453\n",
      "Current iteration=2000, loss=[37167.27854327]\n",
      "||d|| = 4.867995506827081\n",
      "Current iteration=3000, loss=[36308.25353076]\n",
      "||d|| = 2.9075683372718397\n",
      "Current iteration=4000, loss=[42621.81029906]\n",
      "||d|| = 3.227735537374452\n",
      "Current iteration=5000, loss=[36738.79986528]\n",
      "||d|| = 3.4688068495742295\n",
      "Current iteration=6000, loss=[35548.19576156]\n",
      "||d|| = 73.34354263977643\n",
      "Current iteration=7000, loss=[35755.88852453]\n",
      "||d|| = 2.8488321788614126\n",
      "Current iteration=8000, loss=[35693.8320624]\n",
      "||d|| = 4.6848016097339285\n",
      "Current iteration=9000, loss=[35805.72280714]\n",
      "||d|| = 8.860726820609656\n",
      "loss=[35646.20166326]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 4.375367298679311\n",
      "Current iteration=1000, loss=[37491.85311626]\n",
      "||d|| = 9.440218909088205\n",
      "Current iteration=2000, loss=[37338.97383324]\n",
      "||d|| = 2.384179174190181\n",
      "Current iteration=3000, loss=[36815.88368702]\n",
      "||d|| = 2.14871560615275\n",
      "Current iteration=4000, loss=[36421.2019143]\n",
      "||d|| = 4.905401622222086\n",
      "Current iteration=5000, loss=[36159.96754025]\n",
      "||d|| = 6.528945175771364\n",
      "Current iteration=6000, loss=[36251.75027338]\n",
      "||d|| = 7.803600605807253\n",
      "Current iteration=7000, loss=[35928.53207372]\n",
      "||d|| = 6.205502948970045\n",
      "Current iteration=8000, loss=[35454.42237518]\n",
      "||d|| = 2.370387606315075\n",
      "Current iteration=9000, loss=[35383.13707762]\n",
      "||d|| = 8.188450674327937\n",
      "loss=[35333.10702195]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 5.78665259264975\n",
      "Current iteration=1000, loss=[38284.59535239]\n",
      "||d|| = 2.358667740912438\n",
      "Current iteration=2000, loss=[36793.74677578]\n",
      "||d|| = 6.292588219656398\n",
      "Current iteration=3000, loss=[37461.85018573]\n",
      "||d|| = 11.181086400534092\n",
      "Current iteration=4000, loss=[36646.08738742]\n",
      "||d|| = 4.67461939052753\n",
      "Current iteration=5000, loss=[36175.32857416]\n",
      "||d|| = 2.708732419443636\n",
      "Current iteration=6000, loss=[36047.35494618]\n",
      "||d|| = 3.446470057095315\n",
      "Current iteration=7000, loss=[36862.37628933]\n",
      "||d|| = 13.387013871063138\n",
      "Current iteration=8000, loss=[36009.63182003]\n",
      "||d|| = 1.5465939523637353\n",
      "Current iteration=9000, loss=[35906.72704969]\n",
      "||d|| = 5.951407741500836\n",
      "loss=[35852.55258171]\n",
      "Current iteration=0, loss=[42997.30590449]\n",
      "||d|| = 4.850188536967966\n",
      "Current iteration=1000, loss=[38295.74620194]\n",
      "||d|| = 6.646489803141947\n",
      "Current iteration=2000, loss=[36513.62856036]\n",
      "||d|| = 6.822163187712928\n",
      "Current iteration=3000, loss=[36105.9604601]\n",
      "||d|| = 7.328753673801609\n",
      "Current iteration=4000, loss=[36481.94650023]\n",
      "||d|| = 7.757100872141144\n",
      "Current iteration=5000, loss=[35741.51709937]\n",
      "||d|| = 2.220448325876865\n",
      "Current iteration=6000, loss=[35709.82176571]\n",
      "||d|| = 6.648536464741916\n",
      "Current iteration=7000, loss=[35698.88913449]\n",
      "||d|| = 1.540577790538479\n",
      "Current iteration=8000, loss=[36720.28894248]\n",
      "||d|| = 9.417514658738415e-06\n",
      "Current iteration=9000, loss=[35244.71613803]\n",
      "||d|| = 12.101050640415615\n",
      "loss=[35602.58355404]\n",
      "lambda=0.000000, Training accuracy=0.665, Testing accuracy=0.669\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 7.697477838525671\n",
      "Current iteration=1000, loss=[35292.93557716]\n",
      "||d|| = 0.002346437211384327\n",
      "Current iteration=2000, loss=[34841.15435288]\n",
      "||d|| = 6.899626628064337\n",
      "Current iteration=3000, loss=[34568.94442206]\n",
      "||d|| = 2.761035223007865\n",
      "Current iteration=4000, loss=[35685.80831163]\n",
      "||d|| = 13.02100195398589\n",
      "Current iteration=5000, loss=[33799.28271933]\n",
      "||d|| = 9.521419636358816\n",
      "Current iteration=6000, loss=[33918.8382745]\n",
      "||d|| = 4.713121722836796\n",
      "Current iteration=7000, loss=[33802.22141733]\n",
      "||d|| = 24.319940947606167\n",
      "Current iteration=8000, loss=[34266.80171916]\n",
      "||d|| = 6.38465465779208\n",
      "Current iteration=9000, loss=[33625.41475926]\n",
      "||d|| = 7.28070303678797\n",
      "loss=[34168.74304155]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 2.3484649465415206\n",
      "Current iteration=1000, loss=[35428.44607368]\n",
      "||d|| = 6.873583030908697\n",
      "Current iteration=2000, loss=[35494.79850197]\n",
      "||d|| = 21.64558108619054\n",
      "Current iteration=3000, loss=[34892.01040838]\n",
      "||d|| = 1.9918984867534726\n",
      "Current iteration=4000, loss=[34375.88452236]\n",
      "||d|| = 10.857379272223822\n",
      "Current iteration=5000, loss=[34140.55741527]\n",
      "||d|| = 2.9965163463729736\n",
      "Current iteration=6000, loss=[33869.7429246]\n",
      "||d|| = 3.326857952414811\n",
      "Current iteration=7000, loss=[33737.77304396]\n",
      "||d|| = 5.411948745228142\n",
      "Current iteration=8000, loss=[33900.11155035]\n",
      "||d|| = 5.386472387862732\n",
      "Current iteration=9000, loss=[33643.59576681]\n",
      "||d|| = 2.5204053208638806\n",
      "loss=[33577.47997758]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 31.458664752130623\n",
      "Current iteration=1000, loss=[36398.19917624]\n",
      "||d|| = 0.4244904844558011\n",
      "Current iteration=2000, loss=[36651.67384313]\n",
      "||d|| = 3.8188662057634573\n",
      "Current iteration=3000, loss=[34676.23362208]\n",
      "||d|| = 11.631854626085225\n",
      "Current iteration=4000, loss=[34543.44308166]\n",
      "||d|| = 2.5406618624652\n",
      "Current iteration=5000, loss=[34185.28959859]\n",
      "||d|| = 3.3146555742637376\n",
      "Current iteration=6000, loss=[34627.45878319]\n",
      "||d|| = 4.418347832152279\n",
      "Current iteration=7000, loss=[33769.55499432]\n",
      "||d|| = 2.4623708994496165\n",
      "Current iteration=8000, loss=[34257.25291252]\n",
      "||d|| = 3.9165325119512393\n",
      "Current iteration=9000, loss=[33725.09242632]\n",
      "||d|| = 8.093091687344442\n",
      "loss=[34537.30858029]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 4.260991900516669\n",
      "Current iteration=1000, loss=[35262.49082982]\n",
      "||d|| = 3.987599443379084\n",
      "Current iteration=2000, loss=[34811.05143534]\n",
      "||d|| = 2.049642649227609\n",
      "Current iteration=3000, loss=[34976.28948778]\n",
      "||d|| = 1.7958370044263865\n",
      "Current iteration=4000, loss=[34162.976282]\n",
      "||d|| = 14.669879597401849\n",
      "Current iteration=5000, loss=[34300.49927453]\n",
      "||d|| = 4.804853520051711\n",
      "Current iteration=6000, loss=[34408.81513769]\n",
      "||d|| = 2.060941675552196\n",
      "Current iteration=7000, loss=[33777.43879087]\n",
      "||d|| = 5.952103692762321\n",
      "Current iteration=8000, loss=[33650.92466852]\n",
      "||d|| = 2.163948430177532\n",
      "Current iteration=9000, loss=[33628.72322614]\n",
      "||d|| = 3.3784950956735087\n",
      "loss=[33557.71888914]\n",
      "Current iteration=0, loss=[40224.71718225]\n",
      "||d|| = 6.090201644980312\n",
      "Current iteration=1000, loss=[39231.41740729]\n",
      "||d|| = 3.413066115256804\n",
      "Current iteration=2000, loss=[36326.09745501]\n",
      "||d|| = 5.15760520553566\n",
      "Current iteration=3000, loss=[35583.20614301]\n",
      "||d|| = 4.575667146387647\n",
      "Current iteration=4000, loss=[35021.87621805]\n",
      "||d|| = 5.974053920685932\n",
      "Current iteration=5000, loss=[35317.29951619]\n",
      "||d|| = 5.56931187527941\n",
      "Current iteration=6000, loss=[34465.72696735]\n",
      "||d|| = 19.327459688582525\n",
      "Current iteration=7000, loss=[34492.52954157]\n",
      "||d|| = 6.055699132425868\n",
      "Current iteration=8000, loss=[34157.47634231]\n",
      "||d|| = 2.111806364387459\n",
      "Current iteration=9000, loss=[34654.95158926]\n",
      "||d|| = 4.26988467885171\n",
      "loss=[33942.97958636]\n",
      "lambda=0.000000, Training accuracy=0.655, Testing accuracy=0.654\n"
     ]
    }
   ],
   "source": [
    "ws, te_accs, tr_accs, te_losses, tr_losses = [], [], [], [], []\n",
    "lambda_ = 0\n",
    "k = 5\n",
    "\n",
    "for i in x_jet_indexes:\n",
    "    \n",
    "    y_i = y[x_jet_indexes[i]]\n",
    "    tx_i = x[x_jet_indexes[i]]\n",
    "    tx_del = np.delete(tx_i, jet_indexes[i], axis=1)\n",
    "    \n",
    "    # for li in log_indexes:\n",
    "    #   # print(tx_del[:,li].min())\n",
    "    #   tx_del[:,li] = np.apply_along_axis(lambda n: np.log(1 + abs(tx_del[:,li].min()) + n), 0, tx_del[:,li])\n",
    "    \n",
    "    tx_std = standardize(tx_del)[0]\n",
    "    tx_poly = build_poly_matrix_quadratic(tx_std)\n",
    "    tx = np.c_[np.ones((y_i.shape[0], 1)), tx_poly]\n",
    "    \n",
    "    initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    k_indices = build_k_indices(y_i, k, 1)\n",
    "    \n",
    "    te_accs_k, tr_accs_k, te_losses_k, tr_losses_k, ws_k = [], [], [], [], []\n",
    "    \n",
    "    for k_ in range(k):\n",
    "        \n",
    "        test_indices = k_indices[k_]\n",
    "        train_indices = np.setdiff1d(k_indices.flatten(), test_indices)\n",
    "\n",
    "        y_train = y_i[train_indices]\n",
    "        x_train = tx[train_indices]\n",
    "        y_test = y_i[test_indices]\n",
    "        x_test = tx[test_indices]\n",
    "\n",
    "        # Ridge linear\n",
    "        w, loss_tr_k = reg_logistic_regression(y_train, x_train, initial_w, lambda_, 10000, 0.01, method='sgd', ratio=0.5)\n",
    "\n",
    "        # Calculate the loss for test data\n",
    "        loss_te_k = compute_loss(y_test, x_test, w)\n",
    "        \n",
    "        acc_tr_k = compute_accuracy(x_train, w, y_train, mode='logistic')\n",
    "        acc_te_k = compute_accuracy(x_test, w, y_test, mode='logistic')\n",
    "        \n",
    "        te_accs_k.append(acc_te_k)\n",
    "        tr_accs_k.append(acc_tr_k)\n",
    "        te_losses_k.append(np.math.sqrt(2 * loss_te_k))\n",
    "        tr_losses_k.append(np.math.sqrt(2 * loss_tr_k))\n",
    "\n",
    "        ws_k.append(w)\n",
    "        \n",
    "\n",
    "    te_accs.append(np.mean(te_accs_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_accs.append(np.mean(acc_te_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    te_losses.append(np.mean(te_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_losses.append(np.mean(tr_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    ws.append(np.mean(ws_k, axis=0))\n",
    "\n",
    "    print(\"lambda={l:.6f}, Training accuracy={tr:.3f}, Testing accuracy={te:.3f}\".format(\n",
    "           l=lambda_, tr=tr_accs[i] / y_i.shape[0], te=te_accs[i] / y_i.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the logarithm of all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from src.helpers import load_csv_data, standardize, remove_incomplete_columns, predict_labels, create_csv_submission, compute_accuracy\n",
    "from src.logistic.loss import compute_loss\n",
    "from src.logistic.not_req_impl import reg_logistic_regression\n",
    "from src.logistic.gradient import compute_gradient\n",
    "\n",
    "from src.helpers import remove_correlated_columns, get_jet_indexes, jet_indexes, log_indexes, get_all\n",
    "from src.polynomials import build_poly_matrix_quadratic\n",
    "from src.k_fold import build_k_indices\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "y, x_raw, ids = load_csv_data('../data/train.csv')\n",
    "init_col_n = x_raw.shape[1]\n",
    "init_col_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y for logistic regression\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jet_indexes = get_jet_indexes(x_raw)\n",
    "# x_jet_indexes = get_all(x_raw)\n",
    "x = x_raw\n",
    "x_mass = np.zeros(x.shape[0])\n",
    "x_mass[x[:, 0] == -999] = 1\n",
    "x[:, 0][x[:, 0] == -999] = np.median(x[:, 0][x[:, 0] != -999])\n",
    "x = np.column_stack((x, x_mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 21)\n",
      "(77544, 24)\n",
      "(72543, 31)\n"
     ]
    }
   ],
   "source": [
    "for i in x_jet_indexes:\n",
    "    tx_i = x[x_jet_indexes[i]]\n",
    "    tx_del = np.delete(tx_i, jet_indexes[i], axis=1)\n",
    "    print(tx_del.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[69254.41425129]\n",
      "||d|| = 8.452017961819308\n",
      "Current iteration=1000, loss=[48051.30847285]\n",
      "||d|| = 2.342489911071729\n",
      "Current iteration=2000, loss=[47322.49371571]\n",
      "||d|| = 3.7508621461127314\n",
      "Current iteration=3000, loss=[46528.73448391]\n",
      "||d|| = 3.787202275331311\n",
      "Current iteration=4000, loss=[46032.1087465]\n",
      "||d|| = 4.516971082030042\n",
      "Current iteration=5000, loss=[45957.94126128]\n",
      "||d|| = 12.191519043170517\n",
      "Current iteration=6000, loss=[45776.33259439]\n",
      "||d|| = 3.091703034345654\n",
      "Current iteration=7000, loss=[46172.32307359]\n",
      "||d|| = 15.058401186380157\n",
      "Current iteration=8000, loss=[45377.95173017]\n",
      "||d|| = 3.4952133158559335\n",
      "Current iteration=9000, loss=[45262.57208358]\n",
      "||d|| = 7.864668019242488\n",
      "loss=[45218.51880455]\n",
      "lambda=0.000000, Training accuracy=0.772, Testing accuracy=0.772\n",
      "Current iteration=0, loss=[53749.40496934]\n",
      "||d|| = 7.568035474504469\n",
      "Current iteration=1000, loss=[47239.06291202]\n",
      "||d|| = 11.112483510723814\n",
      "Current iteration=2000, loss=[46194.51581023]\n",
      "||d|| = 12.70715864232505\n",
      "Current iteration=3000, loss=[45601.89613915]\n",
      "||d|| = 10.569805962705384\n",
      "Current iteration=4000, loss=[45495.45351353]\n",
      "||d|| = 5.205423743474179\n",
      "Current iteration=5000, loss=[45494.72130259]\n",
      "||d|| = 7.959325940423396\n",
      "Current iteration=6000, loss=[45501.32370019]\n",
      "||d|| = 7.924958946618772\n",
      "Current iteration=7000, loss=[45180.37542584]\n",
      "||d|| = 9.000302397951316\n",
      "Current iteration=8000, loss=[45903.4139017]\n",
      "||d|| = 2.513486981258982\n",
      "Current iteration=9000, loss=[44959.32022009]\n",
      "||d|| = 11.409295568753826\n",
      "loss=[44885.54776955]\n",
      "lambda=0.000000, Training accuracy=0.662, Testing accuracy=0.662\n",
      "Current iteration=0, loss=[50282.97591936]\n",
      "||d|| = 13.327095980609126\n",
      "Current iteration=1000, loss=[44843.70521964]\n",
      "||d|| = 11.650674524898722\n",
      "Current iteration=2000, loss=[42305.99478758]\n",
      "||d|| = 7.9372077176947515\n",
      "Current iteration=3000, loss=[42566.76351093]\n",
      "||d|| = 7.030214651890667\n",
      "Current iteration=4000, loss=[41715.67466991]\n",
      "||d|| = 7.529571704598962\n",
      "Current iteration=5000, loss=[42039.51929556]\n",
      "||d|| = 22.134852887935235\n",
      "Current iteration=6000, loss=[41291.66306701]\n",
      "||d|| = 12.615249426963764\n",
      "Current iteration=7000, loss=[41387.88682072]\n",
      "||d|| = 7.000523393874803\n",
      "Current iteration=8000, loss=[41076.7687425]\n",
      "||d|| = 6.703519472588489\n",
      "Current iteration=9000, loss=[41284.62028916]\n",
      "||d|| = 8.701491022851815\n",
      "loss=[40973.39169706]\n",
      "lambda=0.000000, Training accuracy=0.693, Testing accuracy=0.693\n"
     ]
    }
   ],
   "source": [
    "ws, te_accs, tr_accs, te_losses, tr_losses = [], [], [], [], []\n",
    "lambda_ = 0\n",
    "\n",
    "for i in x_jet_indexes:\n",
    "    \n",
    "    y_i = y[x_jet_indexes[i]]\n",
    "    tx_i = x[x_jet_indexes[i]]\n",
    "    tx_del = np.delete(tx_i, jet_indexes[i], axis=1)\n",
    "    \n",
    "    for li in range(tx_del.shape[1]):\n",
    "       # print(tx_del[:,li].min())\n",
    "       tx_del[:,li] = np.apply_along_axis(lambda n: np.log(1 + abs(tx_del[:,li].min()) + n), 0, tx_del[:,li])\n",
    "    \n",
    "    tx_std = standardize(tx_del)[0]\n",
    "    tx_poly = build_poly_matrix_quadratic(tx_std)\n",
    "    tx = np.c_[np.ones((y_i.shape[0], 1)), tx_poly]\n",
    "    \n",
    "    initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    k_indices = build_k_indices(y_i, k, 1)\n",
    "    \n",
    "    te_accs_k, tr_accs_k, te_losses_k, tr_losses_k, ws_k = [], [], [], [], []\n",
    "    \n",
    "    # Ridge linear\n",
    "    w, loss_tr_k = reg_logistic_regression(y_i, tx, initial_w, lambda_, 10000, 0.01, method='sgd', ratio=0.5)\n",
    "\n",
    "    # Calculate the loss for test data\n",
    "    loss_te_k = compute_loss(y_i, tx, w)\n",
    "\n",
    "    acc_tr_k = compute_accuracy(tx, w, y_i, mode='logistic')\n",
    "    acc_te_k = compute_accuracy(tx, w, y_i, mode='logistic')\n",
    "\n",
    "    te_accs_k.append(acc_te_k)\n",
    "    tr_accs_k.append(acc_tr_k)\n",
    "    te_losses_k.append(np.math.sqrt(2 * loss_te_k))\n",
    "    tr_losses_k.append(np.math.sqrt(2 * loss_tr_k))\n",
    "\n",
    "    ws_k.append(w)  \n",
    "\n",
    "    te_accs.append(np.mean(te_accs_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_accs.append(np.mean(acc_te_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    te_losses.append(np.mean(te_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    tr_losses.append(np.mean(tr_losses_k) * x[x_jet_indexes[i]].shape[0])\n",
    "    ws.append(np.mean(ws_k, axis=0))\n",
    "\n",
    "    print(\"lambda={l:.6f}, Training accuracy={tr:.3f}, Testing accuracy={te:.3f}\".format(\n",
    "           l=lambda_, tr=tr_accs[i] / y_i.shape[0], te=te_accs[i] / y_i.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
